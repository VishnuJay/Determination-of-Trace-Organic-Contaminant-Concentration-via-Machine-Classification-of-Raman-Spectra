{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5953c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import KeyFunctions as me\n",
    "import tensorflow as tf\n",
    "RandState = 92\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b201c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Full Triclosan Dataset\n",
    "df, labels = me.ConstructCombinedChlorDataset()\n",
    "\n",
    "[train, test] = train_test_split(df, random_state = RandState, shuffle = True, train_size = 0.8)\n",
    "\n",
    "y_tn = train.index\n",
    "y_tt = test.index\n",
    "X_tt = test.to_numpy()\n",
    "X_tn = train.to_numpy()\n",
    "\n",
    "#Augment Data to 4000 Spectra\n",
    "X_tnAu, y_tnAu = me.AugmentData(X_tn, y_tn, 4000, df.columns.to_numpy(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdef8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Training Parameters\n",
    "verbose = 1\n",
    "epochsvec = [5, 20, 50]\n",
    "batch_sizevec = [10, 50, 100]\n",
    "epochs = epochsvec[1]\n",
    "batch_size = batch_sizevec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479b91a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 618, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 618, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnS = scaler.transform(X_tnAu)\n",
    "X_ttS = scaler.transform(X_tt)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnS.reshape(X_tnS.shape[0], X_tnS.shape[1], 1)\n",
    "X_tt_p = X_ttS.reshape(X_ttS.shape[0], X_ttS.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)\n",
    "\n",
    "ytruth = tf.argmax(input = y_ttT, axis = 1).numpy()\n",
    "ytruth = encoder.inverse_transform(ytruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374fecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 7s 101ms/step - loss: 0.1610 - accuracy: 0.9341 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 4s 65ms/step - loss: 4.2865e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 6s 101ms/step - loss: 2.1092e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 5s 73ms/step - loss: 1.2618e-04 - accuracy: 1.0000 - val_loss: 9.8651e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 5s 75ms/step - loss: 8.6665e-05 - accuracy: 1.0000 - val_loss: 7.8663e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 6.6139e-05 - accuracy: 1.0000 - val_loss: 6.6867e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 5s 81ms/step - loss: 4.8759e-05 - accuracy: 1.0000 - val_loss: 5.3002e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 5s 78ms/step - loss: 3.6178e-05 - accuracy: 1.0000 - val_loss: 4.5722e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 7s 117ms/step - loss: 3.1069e-05 - accuracy: 1.0000 - val_loss: 4.0391e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 2.6617e-05 - accuracy: 1.0000 - val_loss: 3.5761e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 8s 130ms/step - loss: 2.0251e-05 - accuracy: 1.0000 - val_loss: 3.1641e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 6s 100ms/step - loss: 1.8088e-05 - accuracy: 1.0000 - val_loss: 2.8531e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 1.6032e-05 - accuracy: 1.0000 - val_loss: 2.5465e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 5s 78ms/step - loss: 1.3975e-05 - accuracy: 1.0000 - val_loss: 2.3206e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 6s 90ms/step - loss: 1.1280e-05 - accuracy: 1.0000 - val_loss: 2.2552e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 1.0282e-05 - accuracy: 1.0000 - val_loss: 2.0031e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 6s 88ms/step - loss: 9.2014e-06 - accuracy: 1.0000 - val_loss: 1.8634e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 8.1155e-06 - accuracy: 1.0000 - val_loss: 1.7079e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 5s 85ms/step - loss: 7.3962e-06 - accuracy: 1.0000 - val_loss: 1.6500e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.2160 - accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6000000238418579"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(100 ,input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_split = 0.2, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, SCaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(SCaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ccf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "SCypred = model.predict(X_ttT)\n",
    "SCypred = tf.argmax(input = SCypred, axis = 1).numpy()\n",
    "SCypred = encoder.inverse_transform(SCypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b7e0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.01610883, 0.60764277, ..., 0.4323887 ,\n",
       "         0.40615496, 0.        ],\n",
       "        [0.        , 0.        , 0.24598983, ..., 0.17684364,\n",
       "         0.17249338, 0.        ],\n",
       "        [0.        , 0.        , 0.24294905, ..., 0.17469501,\n",
       "         0.17052875, 0.        ],\n",
       "        ...,\n",
       "        [0.07699407, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.02980497],\n",
       "        [0.13580059, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.05288946],\n",
       "        [0.4041687 , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.15823732]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extract Convolution Feature Maps\n",
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec = list()\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "SCfeature_maps = convlayer.predict(spectra)\n",
    "display(SCfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53fd3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 1236, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1236, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnAu = scaler.transform(X_tnAu)\n",
    "X_tt = scaler.transform(X_tt)\n",
    "\n",
    "#Apply Fourier Transform to Training and Testing Data\n",
    "X_tnf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tnAu)\n",
    "X_ttf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tt)\n",
    "\n",
    "#Combine Real and Imaginary Part of FT in form [real, imaginary]\n",
    "X_tnf = np.append(X_tnf.real, X_tnf.imag, axis = 1)\n",
    "X_ttf = np.append(X_ttf.real, X_ttf.imag, axis = 1)\n",
    "X_tnf= X_tnf.astype('float32')\n",
    "X_ttf= X_ttf.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_tnf)\n",
    "#X_tnf = scaler.transform(X_tnf)\n",
    "#X_ttf = scaler.transform(X_ttf)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnf.reshape(X_tnf.shape[0], X_tnf.shape[1], 1)\n",
    "X_tt_p = X_ttf.reshape(X_ttf.shape[0], X_ttf.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e552db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 11s 159ms/step - loss: 1.9153 - accuracy: 0.8241 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 9s 145ms/step - loss: 0.0159 - accuracy: 0.9991 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 12s 180ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 7.7878e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 10s 165ms/step - loss: 5.2074e-04 - accuracy: 1.0000 - val_loss: 3.8974e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 10s 156ms/step - loss: 3.0022e-04 - accuracy: 1.0000 - val_loss: 2.4308e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 11s 174ms/step - loss: 1.6001e-04 - accuracy: 1.0000 - val_loss: 1.5080e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 11s 177ms/step - loss: 1.1896e-04 - accuracy: 1.0000 - val_loss: 1.0783e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 7.2017e-05 - accuracy: 1.0000 - val_loss: 7.4536e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 10s 149ms/step - loss: 5.9001e-05 - accuracy: 1.0000 - val_loss: 5.1139e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 4.9510e-05 - accuracy: 1.0000 - val_loss: 4.0294e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 9s 144ms/step - loss: 3.4524e-05 - accuracy: 1.0000 - val_loss: 3.4529e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 7s 110ms/step - loss: 2.9163e-05 - accuracy: 1.0000 - val_loss: 2.8702e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 9s 141ms/step - loss: 2.1983e-05 - accuracy: 1.0000 - val_loss: 2.5621e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 2.0002e-05 - accuracy: 1.0000 - val_loss: 2.2912e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 9s 143ms/step - loss: 2.0423e-05 - accuracy: 1.0000 - val_loss: 1.7477e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 1.3463e-05 - accuracy: 1.0000 - val_loss: 1.5406e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 9s 145ms/step - loss: 1.5823e-05 - accuracy: 1.0000 - val_loss: 1.2589e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 7s 107ms/step - loss: 3.0890e-05 - accuracy: 1.0000 - val_loss: 1.2385e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 9s 144ms/step - loss: 1.3434e-05 - accuracy: 1.0000 - val_loss: 1.0617e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 8.4072e-06 - accuracy: 1.0000 - val_loss: 9.1456e-06 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 5.5214 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(100 ,input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.2, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, FTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(FTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54173aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "FTypred = model.predict(X_ttT)\n",
    "FTypred = tf.argmax(input = FTypred, axis = 1).numpy()\n",
    "FTypred = encoder.inverse_transform(FTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0580f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2_input (InputLayer)  [(None, 1236, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1236, 64)          128       \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.       , 25.360638 ,  0.       , ...,  0.       ,\n",
       "         11.767738 ,  9.705614 ],\n",
       "        [27.345022 ,  0.       ,  3.3393743, ...,  5.063018 ,\n",
       "          0.       ,  0.       ],\n",
       "        [17.95174  ,  0.       ,  2.1872292, ...,  3.3201008,\n",
       "          0.       ,  0.       ],\n",
       "        ...,\n",
       "        [29.669846 ,  0.       ,  3.6245286, ...,  5.4943876,\n",
       "          0.       ,  0.       ],\n",
       "        [12.944369 ,  0.       ,  1.5730432, ...,  2.3909864,\n",
       "          0.       ,  0.       ],\n",
       "        [21.45894  ,  0.       ,  2.6174092, ...,  3.970859 ,\n",
       "          0.       ,  0.       ]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "convlayer.summary()\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "FTfeature_maps = convlayer.predict(spectra)\n",
    "display(FTfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756a0320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 1024, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1024, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply Welsh-Hadamard Transform to Training and Testing Data\n",
    "from sympy.discrete.transforms import fwht, ifwht\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnAu = scaler.transform(X_tnAu)\n",
    "X_tt = scaler.transform(X_tt)\n",
    "\n",
    "X_tnh = np.apply_along_axis(fwht, axis=1, arr=X_tnAu)\n",
    "X_tth = np.apply_along_axis(fwht, axis=1, arr=X_tt)\n",
    "X_tnh = X_tnh.astype('float32')\n",
    "X_tth = X_tth.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_tnh)\n",
    "#X_tnh = scaler.transform(X_tnh)\n",
    "#X_tth = scaler.transform(X_tth)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnh.reshape(X_tnh.shape[0], X_tnh.shape[1], 1)\n",
    "X_tt_p = X_tth.reshape(X_tth.shape[0], X_tth.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e029d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 8s 112ms/step - loss: 0.8665 - accuracy: 0.9087 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.9038e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 3.1824e-04 - accuracy: 1.0000 - val_loss: 1.1694e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 1.9936e-04 - accuracy: 1.0000 - val_loss: 7.2651e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 6s 94ms/step - loss: 1.2726e-04 - accuracy: 1.0000 - val_loss: 5.1382e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 8.1966e-05 - accuracy: 1.0000 - val_loss: 3.8158e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 6s 95ms/step - loss: 7.0717e-05 - accuracy: 1.0000 - val_loss: 2.9389e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 4.6317e-05 - accuracy: 1.0000 - val_loss: 2.1621e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 7s 104ms/step - loss: 3.4013e-05 - accuracy: 1.0000 - val_loss: 1.8058e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 7s 106ms/step - loss: 2.6621e-05 - accuracy: 1.0000 - val_loss: 1.4282e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 8s 118ms/step - loss: 2.3173e-05 - accuracy: 1.0000 - val_loss: 1.2727e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 6s 98ms/step - loss: 2.1108e-05 - accuracy: 1.0000 - val_loss: 1.0456e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 9s 143ms/step - loss: 1.5757e-05 - accuracy: 1.0000 - val_loss: 8.8578e-06 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 6s 99ms/step - loss: 1.7104e-05 - accuracy: 1.0000 - val_loss: 7.6802e-06 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 9s 136ms/step - loss: 1.2362e-05 - accuracy: 1.0000 - val_loss: 6.3399e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 6s 100ms/step - loss: 8.9231e-06 - accuracy: 1.0000 - val_loss: 5.7404e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 8.3403e-06 - accuracy: 1.0000 - val_loss: 5.2861e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 7s 103ms/step - loss: 8.2496e-06 - accuracy: 1.0000 - val_loss: 4.7448e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 9s 134ms/step - loss: 6.2408e-06 - accuracy: 1.0000 - val_loss: 4.1349e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 7s 103ms/step - loss: 6.6128e-06 - accuracy: 1.0000 - val_loss: 3.9401e-06 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 3.1888 - accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.699999988079071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(100 ,input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = 'min',\\\n",
    "                                           patience = 2, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.2, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, HTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0d1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000252579CB820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "#Make Prediction\n",
    "HTypred = model.predict(X_ttT)\n",
    "HTypred = tf.argmax(input = HTypred, axis = 1).numpy()\n",
    "HTypred = encoder.inverse_transform(HTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d13ca327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4_input (InputLayer)  [(None, 1024, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1024, 64)          128       \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000252657FCE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.9105049e+01, 0.0000000e+00, 2.2197769e+00, ...,\n",
       "         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "        [0.0000000e+00, 2.6019096e-01, 0.0000000e+00, ...,\n",
       "         4.7472331e-01, 2.0984428e-01, 1.2972847e-01],\n",
       "        [0.0000000e+00, 2.5155574e-01, 0.0000000e+00, ...,\n",
       "         4.5589703e-01, 2.0249116e-01, 1.2414062e-01],\n",
       "        ...,\n",
       "        [1.8861619e-01, 0.0000000e+00, 1.2501414e-02, ...,\n",
       "         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "        [1.3911340e+00, 0.0000000e+00, 1.5281792e-01, ...,\n",
       "         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "        [0.0000000e+00, 8.3098996e-01, 0.0000000e+00, ...,\n",
       "         1.7191615e+00, 6.9589299e-01, 4.9909174e-01]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "convlayer.summary()\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "HTfeature_maps = convlayer.predict(spectra)\n",
    "display(HTfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0107edb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10-4', '10-7', '10-7', '10-4', '10-5', '10-5', '10-5', '10-5',\n",
       "       '10-3', '10-4'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6000000238418579"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.699999988079071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "display(SCypred)\n",
    "CMSC = confusion_matrix(ytruth, SCypred, labels = labels)\n",
    "CMFT = confusion_matrix(ytruth, FTypred, labels = labels)\n",
    "CMHT = confusion_matrix(ytruth, HTypred, labels = labels)\n",
    "display(SCaccuracy, FTaccuracy, HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c1b1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA38AAAFZCAYAAAAo3ZaNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA6IElEQVR4nO3debwcVZn/8e8XwhIg+2bYJQYUFCMmiChKEJ0AyqKiMC6IjIgzgDqowOgILvxE0FFRRwWFILIqq4Asg4TgFjZDEtkCmARIyEoWskGS5/dHnRuazu17+97uvtV96/N+ve7rdtf6nDpdp/qpOlXtiBAAAAAAoHfbLO8AAAAAAACNR/IHAAAAAAVA8gcAAAAABUDyBwAAAAAFQPIHAAAAAAVA8gcAAAAABUDyBwDo9Wz/3PZ/5x1Hvdjew/bfba+wfWre8QDNzvaBtp+tYrqzbf+mJ2KqsP6q4syD7V1th+0+ecfSHtsjbE9O7eL3846nWZH8oS5sz7K92vaLtp+3PdH2dg1e50TbL6WdfIXtGba/Y3tAF+M+uJFxAqisrO1o+9u+3uuJiJMi4lu1Lsf2zmWxhu2VJe8PqEe8VfiKpEkR0S8iLuihdQI9yvaZtm8tGzazwrBjeja6jesO268rG5ZrApmHsnZxQ1m7/rEeCuNESYsk9Y+I03ponS2H5A/19IGI2E7SGElvkXRmD6zzvIjoJ2mYpOMl7Sfpz7a37YF1A6iPD0TEdiV/c+u5cNub1zDvq85wR8Sc0ljT4DeXDLu30rx1toukf3RnxmY9aw+0Y7Kkd7Ttw7ZfI2kLSfuUDXtdmhZ10tV2oqxdnKNXt+uXd3e5XbSLpEciIro6Y5HaRZI/1F1EPC/pdmVJoCTJ9hm2n0pX6B6xfVTJuNm235pefzydRdszvf832zdUsc41EXG/pMMlDVGWCMr2KNt/tL3Y9iLbl9semMZdJmlnSb9PZ6a+kob/Nl29XJa6D+xVj+0CoHq2t7L9Q9tz098PbW+Vxn3K9p/Kpt949j31CviZ7Vttr5Q0Pg37dsn077c91fZS23+xvXfJuFm2T7c9TdLKar8UpLj+bPsHtpdIOrujNqhkXV+yPS21OVfb3jqNG2r75hTjEtv32t7M9h8ljZf0k9R27W57gO1f216Y2tSv2d6sg7gm2v5f239Iy/iz7dek7fyC7cdsv6VblQfUz/3Kkr0x6f27JN0t6fGyYU9FxFzbx9t+NH3XeNr2ZystOO3jz6VpH7f9npLRW6b9aYXtf9geW0shbP/I9jO2l9t+0CU9BGz3TfvjC7YfkTSubN6Ovj+V7ttLU5n3T8Ofsb3A9nEl0x/mrLv48jT+7JJxbV06T7A9R9IfbW9u+3up7Xpa0mHdKPuBtp9N2/t5SZfYHpTatoWp3Dfb3rFknkm2v5XKtsL2HbaHpnFb2/5NalOX2r7fWXfPiZKOk/SV1KYd7I6PI+3Fdbaz74C/SeudntrXM9O2fMb2+7q6DZoNyR/qLu3Ah0h6smTwU5IOkDRA0jck/cb2yDTuHkkHptfvkvS0pHeXvL+n2nVHxApJd6Z1SZIlfUfS9pLeIGknSWenaT+hV5+dOi/N8wdJoyUNl/SQpI1nrAD0mK8qu5I/RtKbJe0r6WtdmP9fJZ0jqZ+k8kRxH0kXS/qsspNFv5B0U9uXguRYZV90BkbEui6s923K2rDhaf0V26ASH5E0QdJrJe0t6VNp+GmSnlXWs2GEpP+SFBFxkKR7JZ2c2q4nJP1YWfu6m7L285NKJ8EqxNW23q9JGippraS/Kmvzhkr6naT/6UK5gbqLiJckTVH2XUDp/73K9unSYW1X/RZIer+k/so+/z9I+/ur2N5D0smSxqXeQ/8iaVbJJIdLukrSQEk3SfpJjUW5X1lbNljSFZJ+63SSR9JZkkalv39RlsCU6uj7k5Tt29OUtWVXpLjHKbsa+nFlJ4naeimsVNY2DFTWvn3O9pFl63u3srbqXyR9Rtn2fIuksZI+3I2yS9JrlJV9F2VdMzeTdEl6v7Ok1dp0G/+rsjocLmlLSV9Kw49Tti12SmU+SdLqiPiUsu9r56V28f/U+XGkPC5J+oCkyyQNkvR3ZRczNpO0g6RvKjtetDSSP9TTDbZXSHpGWQN8VtuIiPhtRMyNiA0RcbWkmcp2QilL7tqSvQOUfVFqe/9udSH5S+Yq25kVEU9GxJ0RsTYiFir7MvPujmaOiIsjYkVErFX2Je3N7sJ9hAC67IZ0BnepX7nS/zFJ34yIBWnf/YakT3RhmTdGxJ9Tm7OmbNxnJP0iIqZExPqIuFRZ8rNfyTQXRMQzEbG6i2WZGxE/joh1EbG6yjbogtQ+LpH0e71yReNlSSMl7RIRL0fEve11Z3LW/e2jks5MbdcsSd/Xq7fXq+JKw66PiAfT9rle0pqI+HVErJd0tbIvfEDe7tErid4BypK/e8uG3SNJEXFLRDwVmXsk3aFXTgaXWi9pK0l72t4iImZFxFMl4/8UEbemfeEyZYlDRx4qacOWSjqjdGRE/CYiFqf97/tp3Xuk0R+RdE5ELImIZyRdUDZvR9+fJOmfEXFJyX67k7K2c21E3CHpJWWJoCJiUkRMT8uaJulKbdoenR0RK1M78RFJP0xt4RJl38+6Y4Oks1JMq9O2uDYiVqWT9ue0E8clEfFEiuMavbpdHCLpdan9fjAilldYb2fHkVfFlYbdGxG3p5N+v1V28u3ciHhZWWK9q0t6b7Qikj/U05HpDNqBkl6v7OyxJMn2J/1KF6ulkt5YMv4eSQc467e/ubLG6x22d1V2dmdqF+PYQdKStN7htq9KXTuWS/pNaVzlUheHc1MXi+V65UxgxXkA1OzIiBiY/o5Mw7aXNLtkmtlpWLWe6WDcLpJOK/uytlPZ8juav+r1VtkGPV/yepWktrP05yvrQXFH6s51hto3VNmZ8fLttUOluJL5Ja9Xt/O+oQ/tAqo0WdI7bQ+SNCwiZkr6i6T907A3pmlk+xDbf3PWTXqppEPVzvE7Ip6U9AVlJ3gXpH20dP8v3ye3dsfdv/cpacMGSjq3dKTt05x1R12W4hpQEtf2evX+Obts3o6+P0mb7reKiHb3Zdtvs3136m65TNlVs/LtUxpLh7F1wcLSk3C2t7H9C2dd1Jcrq7+BfvX92ZXaxcuUXY27KnXlPM/2FhXW29lx5FVxJeXbblFKrNveSy3eNpL8oe7S2baJkr4nSbZ3kXSRsi4WQ1LDOENZd6i2RniVpFMlTU5ngZ5Xdgn+TxGxodp1p64NBys7KyhlZ6lC0t4R0V9ZFwiXhlu2iH+VdERaxgBJu7YtutoYANTFXGVJWpud0zAp67q0TduIdOKoXEc3/D+j7Ez7wJK/bSLiyirn70j5fJ21QZUXlF3FOy0idlPWFek//er7ktosUnY2vHx7PddBXECr+Kuy4/GJkv4sSelKz9w0bG5E/DN1275W2XePEem7xq2qsL9FxBUR8U5l+01I+m4jgnd2f9/pyq6iDUpxLSuJa56yk09tdi6Zt8PvT91whbJurDtFxABJP29nWaVtRcXYuqi8/TlN2ZXPt6V2se0qbqflSr0gvhERe0raX1m31E9WmLyj40h7cRUCyR8a5YeS3mt7jKRtle1gCyXJ9vHKzlyVukdZ49bWxXNS2fsOpZt63yrpBkkvKOtLLmX3+7woaantHSR9uWzW+crukVHJ9GslLVb25fL/VbN+AHV3paSv2R6WbvT/urKrZpL0sKS9bI9J982c3cVlXyTppHQW3La3dfYghH51i/4VnbVBFTl7KM3rbFvScmVd1daXT5fOSl8j6Rzb/dIXxv/UK9sLaFmpO94Dyj7T95aM+lMa1na/35bKulMulLTO9iGS2n04h7PfyTwoJYxrlF3R2WTfqpN+ktaluPrY/rqyexLbXCPpTGcPQdlR0ikl46r5/tTVWJZExBrb+yo74d2RaySdanvHdJW1Uu+D7sSxWlm7OFgltwl1xvZ4229KVwmXKzvxVanuOjqOFBbJHxoi9a3+taT/johHlN1/8ldlydablM7elbhHWWMwucL7Sr6S7jNcktb3oKT9I2JlGv8NSfsoO8t2i6Tryub/jrKGYantL6VlzFZ2xvwRSX+rtswA6urbyr7wTZM0XdmDSL4tSZE94OSbkv5P2f0vf6qwjHZFxAPK7vv7ibKTRU/qlYes1FtnbVBHRisr44vK2s//jYhJFaY9RdkV0aeVbY8rlD3UBugN7lH24I/Sff3eNGyytPGBb6cqS1heUJbY3FRheVsp65q5SFlPo+HKHqjUCLcre5DcE8q+X6zRq7tSfiMN/6eyexQvaxtR5fenrvh3Sd9M35u+rmxbdeSiFP/DytrgrrRfHfmhpL7Ktv/fJN3WhXlfo+yBVMslParss1Epoat4HCkyt3PvOAAAAACgl+HKHwAAAAAUAMkfAAAAABQAyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyV9B2V5ve6rtGbZ/a3ubzuequKyJtj+cXv/S9p4dTHug7f27sY5Z6TdayodvZ/sXtp+y/Q/bk22/ravLb2e5u9qekV6PtX1Be/HbPsl2pR8XBdANtE+dro/2CcgBbVOn66NtagF98g4AuVkdEWMkyfblkk6S9D9tI21vnn44uEsi4t86meRAZb9Z9ZeuLruCXyr7bZzREbHB9m6S3lCnZUva+JtgD6S3B6ok/oj4eT3XBUAS7VPVaJ+AHkXbVCXapubFlT9I2Q+lvi6dmbnb9hWSptve3Pb5tu+3Pc32ZyXJmZ/YfsT2Lcp+HFVp3CTbY9PrCbYfsv2w7bts76qsofxiOnN2gO1htq9N67jf9jvSvENs32H777Z/IcnlQdseJeltkr4WERskKSKejohb0vj/TGfnZtj+Qhq2q+1HbV+UznbdYbtvGvfWFOtfJf1HyXoOtH1zhfjPdvbj8LI9xvbf0ra63vagkm3yXdv32X7C9gH1qjigAGifRPsENCHaJtE2tSKSv4Kz3UfSIZKmp0H7SvpqROwp6QRJyyJinKRxkj5j+7WSjpK0h6Q3SfqMpE26ItgeJukiSR+KiDdLOjoiZkn6uaQfRMSYiLhX0o/S+3GSPqTsbJQknSXpTxHxFkk3Sdq5nfD3kjS1vbNstt8q6XhlDdx+Kfa3pNGjJf00IvaStDStV5IukXRqRLy9vW1VIf5Sv5Z0ekTsrWx7nlUyrk9E7CvpC2XDAVRA+0T7BDQj2ibaplZGt8/i6mt7anp9r6RfKWuI7ouIf6bh75O0t1OfdEkDlO3875J0ZWo45tr+YzvL30/S5LZlRcSSCnEcLGlPe+PJqf62+6V1fDDNe4vtF7pYvndKuj4iVkqS7eskHaCsMfxnRLSV/UFJu9oeIGlgRNyThl+mrGGvSjvzXyrptyWTXFe6vi6WBSga2qcM7RPQXGibMrRNLYzkr7g29ltvkxqRlaWDJJ0SEbeXTXeopOhk+a5iGim7+vz2iFjdTiydzf8PSW+2vVlb14Wy9VeytuT1ekl9uxBvd7Wtc73Y74DO0D5laJ+A5kLblKFtamF0+0RHbpf0OdtbSJLt3W1vK2mypGNSv/aRksa3M+9fJb07dXWQ7cFp+ApJ/Uqmu0PSyW1vbI9JLydL+lgadoikQeUriIinlN1M/A2nFs/2aNtHpPmPtL1NivkoZWfp2hURSyUts/3ONOhjFSYtj79t/mWSXijpk/4JSfeUTwegbmifNkX7BOSPtmlTtE1NhOQPHfmlpEckPeTs0b2/UHbm5XpJM5X1zf6Z2tlRI2KhpBMlXWf7YUlXp1G/l3RU202/kk6VNDbd6PuIspuCJekbkt5l+yFlXSjmVIjx3yS9RtKTtqcr6ys/NyIekjRR0n2Spkj6ZUT8vZPyHi/pp85uWl5dYZry+EsdJ+l829MkjZH0zU7WB6D7aJ82RfsE5I+2aVO0TU3EEY28WgsAAAAAaAZc+QMAAACAAiD5AwAAAIACIPkDAAAAgALoVY9N7WfHsLyDqNHqzTdX3/Wb/O5mS1m3/fYaOGRI3mHUZOnSpRo4cGDeYdRkyZIlGjRoUNujn1vW9OnTF0W09q7dv3//2HXXXfMOoybzZ8+Wli/PO4yarOvbVzu87nV5h1GTl156SevWrdM222yTdyg1Wb58ufr37593GDVZtWqVnnrqqZZvnwZv6dixb95RdN/K9VKfzaStWvtQp+WDdlb//gPyDqMmy5Yt04ABrV2GF154QYMGbfKQ1JayZs0azZw5s2Lb1KuSvxHKHrHUyh4aNEj7LFqUdxg1ee4rX9HHPv/5vMOoycSJE/WpT30q7zBqcuGFF+rTn/60+vRp7d3c9uy8Y6jVyJEjNW3atLzDqMnXjj5aW/zud3mHUZPn3vAGXfjgg3mHUZM5c+Zo/vz5GjduXN6h1OTGG2/UEUcckXcYNZk0aZLGjx/f8u3TbttJDxycdxTd99fF0vCtpVHb5h1JbW4+5nt6/wePzjuMmlx11VU65phj8g6jJr/61a90wgkn5B1GTaZPn6699967YttEt08AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKICGJX+2L7a9wPaMkmGDbd9pe2b6P6jCvN+yPc32VNt32N6+UXECAAAAQBE08srfREkTyoadIemuiBgt6a70vj3nR8TeETFG0s2Svt6oIAEAAACgCBqW/EXEZElLygYfIenS9PpSSUdWmHd5ydttJUW94wMAAACAIunTw+sbERHzJCki5tkeXmlC2+dI+qSkZZLG91B8AAAAANArNe0DXyLiqxGxk6TLJZ1caTrbJ9p+wPYDyytNBAA9rLRtWrZsWd7hAMBGpe3TwrV5RwOgJ/V08jff9khJSv8XpNeXpIe73NrOPFdI+lClBUbEhRExNiLG9m9IyADQdaVt04ABA/IOBwA2Km2fhm2VdzQAelJPJ383STouvT5O0o2SFBHHR8SYiDhUkmyPLpnncEmP9WiUAAAAANDLNOyeP9tXSjpQ0lDbz0o6S9K5kq6xfYKkOZKOrjD7ubb3kLRB0mxJJzUqTgAAAAAogoYlfxFxbIVR76li3ordPAEAAAAAXde0D3wBAAAAANQPyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyR8AAAAAFADJHwAAAAAUAMkfAAAAABRAn7wDqKeXJT2TdxA1mr9uXcuXYeHixZo5c2beYdTk+eefb/kyzJ8/XzNnzlSfPr1qN29J69ata/nP0wsrVmjzvIOo0bKXXmr5enjuuee0aNEiDRw4MO9QajJv3ryWr4ulS5fmHUJdrF0vzVyRdxTdN+dF6cV10oYNeUdSm2fnzm35faI3fXdqZbNnz+5wfK/6Vrj17rvrTVdfnXcYNTntve/Viq23zjuMmpy6dq1WrlyZdxg1Wb9+fcuX4eWXX9bKlStJ/ppAb/g8vfTyy9qqxdumDVLL18Pq1au1atWqli/HmjVrWr4MK1a0cMZUYv0Ob9DK71yRdxjdtubhh/Xrz56ogW7tzmy7Htj6+0Tb945Wtm7dupYvQ2fx96pvhX379tWYMWPyDqMmfbbYQmvWrMk7jJpsv/32LV8PU6dObfky3HfffRozZgzJXxPYaqutWv7zNGLwYG3R4m3TS1tu2fL1MHjwYM2fP7/lyzF79uyWL0NvufK3zTbbtHRdrF69Wg+tfUmD8w6kRrvttltL14MkPfbYYy1fhgcffLDly7D55h3302nt0yQAAAAAgKqQ/AEAAABAAZD8AQAAAEABkPwBAAAAQAGQ/AEAAABAAZD8AQAAAEABkPwBAAAAQAGQ/AEAAABAAZD8AQAAAEABkPwBAAAAQAGQ/AEAAABAAZD8AQAAAEABkPwBAAAAQAE0LPmzfbHtBbZnlAwbbPtO2zPT/0GdLONLtsP20EbFCQAAAABF0MgrfxMlTSgbdoakuyJitKS70vt22d5J0nslzWlUgAAAAABQFH2qmcj2/pJ2LZ0+In7d0TwRMdn2rmWDj5B0YHp9qaRJkk6vsIgfSPqKpBuriREAAAAAUFmnyZ/tyySNkjRV0vo0OCR1mPxVMCIi5klSRMyzPbzCOg+X9FxEPGy7G6sBAAAAAJSq5srfWEl7RkQ0OhhJsr2NpK9Kel+V058o6URJ2nHHHRsYGQBUr7RtGjFiRM7RAMArStunnXfeOedoAPSkau75myHpNXVa33zbIyUp/V+QXl9ie6rtW5VdZXytpIdtz5K0o6SHbLcbQ0RcGBFjI2LskCFD6hQmANSmtG0aMGBA3uEAwEal7dOwYcPyDgdAD6rmyt9QSY/Yvk/S2raBEXF4N9Z3k6TjJJ2b/t+YlnV82XQbu4OmBHBsRCzqxvoAAAAAAKou+Tu7Owu2faWyh7sMtf2spLOUJX3X2D5B2VM8j+7OsgEAAAAAXdNp8hcR99geIWlcGnRfRCyoYr5jK4x6TxfiU0Ts2pXpAQAAAACb6vSeP9sfkXSfsqt0H5E0xfaHGx0YAAAAAKB+qun2+VVJ49qu9tkeJun/JP2ukYEBAAAAAOqnmqd9blbWzXNxlfMBAAAAAJpENVf+brN9u6Qr0/uPSrq1cSEBAAAAAOqtmge+fNn2hyS9Q5IlXRgR1zc8MgAAAABA3VRz5U8Rca2kaxscCwAAAACgQSomf7b/FBHvtL1CUpSOkhQR0b/h0QEAAAAA6qJi8hcR70z/+/VcOAAAAACARqjmd/4uq2YYAAAAAKB5VfOTDXuVvrHdR9JbGxMOAAAAAKARKiZ/ts9M9/vtbXt5+lshab6kG3ssQgAAAABAzSomfxHxnXS/3/kR0T/99YuIIRFxZg/GCAAAAACoUUdP+3x9RDwm6be29ykfHxEPNTQyAAAAAEDddPQ7f6dJ+oyk77czLiQd1JCIarBhwwatWrUq7zBqsn79+rxDqNlLL73U8vWwZs2ali9DWz306VPVz3migSKi5T9Pa9ete9Vv/rSil9evb/l6WL16tVatWtXy5egtbWxv0OrfnVavXq2XJLV6bbTt262sN3z/W7t2bcuXobP4O/qph8+k/+PrHFPDrFq1SlOmTMk7jJqccsop2n///fMOoybXf/7z+tU55+QdRk3++a53acoee+QdRk1mz56tKVOmkPw1gbVr17Z829T3jW/UgaeckncYNbnllltavh4WLlyoBRecqA0t/ku7T771c5oyfHjeYdRk9uzZeYdQFytXrmzp/eLRRx/V2ydO1I477ZR3KDX529/+1tL1IEmzfneBplx1at5h1OTZ7cZqyhvekHcYNXnqqac6HN9Rt88PdjRjRFzXzZgaZrvtttP48S2Tq7ZrzZo1Ouigpruo2iXTBgzQm2bMyDuMmqwfOrTlP0szZ87U+PHjSf6awNZbb93yn6dnnnmm5dumxx9/vOXrYc6cOZr/65c1bpvWPjO9fK89W74ubOcdQl3069evpeti66231vDhwzVq1Ki8Q6nJqlWrWroeJGn+lYM0fumyvMOoydPDB7R8PQwdOrTD8R19K/xA+j9c0v6S/pjej5c0SVLTJX8AAAAAgPZ11O3zeEmyfbOkPSNiXno/UtJPeyY8AAAAAEA9VPMj77u2JX7JfEm7NygeAAAAAEADVHMz0CTbt0u6UtlTPo+RdHdDowIAAAAA1FWnyV9EnJwe/nJAGnRhRFzf2LAAAAAAAPVU1WMA05M9ecALAAAAALSoTu/5s/1B2zNtL7O93PYK28t7IjgAAAAAQH1Uc+XvPEkfiIhHGx0MAAAAAKAxqnna53wSPwAAAABobdVc+XvA9tWSbpC0tm1gug8QAAAAANACqkn++ktaJel9JcNCPAAGAAAAAFpGNT/1cHxPBAIAAAAAaJxqnva5o+3rbS+wPd/2tbZ37IngAAAAAAD1Uc0DXy6RdJOk7SXtIOn3aViHbF+cEsYZJcMG274z/XTEnbYHVZj3bNvP2Z6a/g6trjgAAAAAgPZUk/wNi4hLImJd+psoaVgV802UNKFs2BmS7oqI0ZLuSu8r+UFEjEl/t1axPgAAAABABdUkf4tsf9z25unv45IWdzZTREyWtKRs8BGSLk2vL5V0ZFeCBQAAAAB0TzXJ36clfUTS85LmSfpwGtYdIyJiniSl/8M7mPZk29NS99F2u4cCAAAAAKrTafIXEXMi4vCIGBYRwyPiyIiY3eC4fiZplKQxyhLO71ea0PaJth+w/cDixZ1ekASAHlHaNi1btizvcABgo9L2aeHChXmHA6AHVUz+bJ9n+6R2hn/R9ne7ub75tkem5YyUtCC9viQ92OVWSYqI+RGxPiI2SLpI0r6VFhgRF0bE2IgYO2TIkG6GBQD1Vdo2DRgwIO9wAGCj0vZp2LBqHuMAoLfo6Mrf+yVd2M7wH0k6rJvru0nScen1cZJulLLfEkwPdjlU2pgYtjlK0gwBAAAAALqtox95j3TlrXzgBtvubMG2r5R0oKShtp+VdJakcyVdY/sESXMkHV1h9vNsj5EUkmZJ+mxn6wMAAAAAVNZR8rfK9uiImFk60PZoSas7W3BEHFth1HuqmPcTnU0DAAAAAKheR8nf1yX9wfa3JT2Yho2VdKakLzQ4LgAAAABAHVVM/iLiD7aPlPRlSaekwTMkfSgipvdAbAAAAACAOunoyp8iYoZeeUALAAAAAKBFVfMj7wAAAACAFkfyBwAAAAAFQPIHAAAAAAVQ8Z4/2z9W9jt77YqIUxsSEQAAAACg7jp64MsDPRYFAAAAAKChOvqph0t7MhAAAAAAQON0+FMPkmR7mKTTJe0paeu24RFxUAPjAgAAAADUUTUPfLlc0qOSXivpG5JmSbq/gTEBAAAAAOqsmuRvSET8StLLEXFPRHxa0n4NjgsAAAAAUEeddvuU9HL6P8/2YZLmStqxcSEBAAAAAOqtmuTv27YHSDpN0o8l9Zf0xYZGBQAAAACoq06Tv4i4Ob1cJml8Y8OpzYoVK3TbbbflHUZN7r//ftnOO4yaXLJsmeYOHpx3GDXZb8ECvb7FP0uPP/64brvtNvXpU805HjTS6tWrW75teuKJJ1q+DA9NmqTPnnVW3mHU5MX16/XWM8/S4jfsmXcoNZn6wIPaqsU/T7Nnz847hLpYvnx5S+/bjz32mLbbbjvNnDkz71Bq8tD3/0N9fv7pvMOoyaMvbK7bBrT297+ZG5a09P4gSU8//XSH46t52uclaufH3tO9f02lX79+mjBhQt5h1CQiWr4M3/72t7Voxoy8w6jJsOHDW74e5syZowkTJpD8NYG+ffu2/OdpwYIFLV+Gv999t15auDDvMGqyVNIB7zpQ4/bdN+9QarJ23fqW/zxNmjQp7xDqon///i1dFwMGDNDw4cM1atSovEOpybqL+mjC5i3ePvXZTRO2fD7vMGry3PDBLb0/SNL06dM7HF/Nt8KbS15vLekoZff9AQAAAABaRDXdPq8tfW/7Skn/17CIAAAAAAB1V81PPZQbLWnnegcCAAAAAGicau75W6FX3/P3vKTTGxYRAAAAAKDuqun22a8nAgEAAAAANE6n3T5t31XNMAAAAABA86p45c/21pK2kTTU9iBJbT8+11/S9j0QGwAAAACgTjrq9vlZSV9Qlug9qFeSv+WSftrYsAAAAAAA9VQx+YuIH0n6ke1TIuLHPRgTAAAAAKDOqvmphw22B7a9sT3I9r83LiQAAAAAQL1Vk/x9JiKWtr2JiBckfaZhEQEAAAAA6q6a5G8z2233+8n25pK2bFxIAAAAAIB66/R3/iTdLuka2z9X9mPvJ0m6raFRAQAAAADqqporf6dLukvS5yT9R3r95c5msn2x7QW2Z5QMG2z7Ttsz0/9BHcx/iu3Hbf/D9nlVxAkAAAAAqKDT5C8iNkTEzyPiwxHxIUn/kFTN0z8nSppQNuwMSXdFxGhlSeQZ7c1oe7ykIyTtHRF7SfpeFesDAAAAAFRQzZU/2R5j+7u2Z0n6lqTHOpsnIiZLWlI2+AhJl6bXl0o6ssLsn5N0bkSsTctaUE2cAAAAAID2Vbznz/buko6RdKykxZKuluSIGF/D+kZExDxJioh5todXmG53SQfYPkfSGklfioj7a1gvAAAAABRaRw98eUzSvZI+EBFPSpLtL/ZIVFlcgyTtJ2mcsgfO7BYRUT6h7RMlnShJO+64Yw+FBwAdK22bRowYkXM0APCK0vZp5513zjkaAD2po26fH5L0vKS7bV9k+z2S3MH01Zhve6Qkpf8L0utLbE+1fWua7llJ10XmPkkbJA1tb4ERcWFEjI2IsUOGDKkxPACoj9K2acCAAXmHAwAblbZPw4YNyzscAD2oYvIXEddHxEclvV7SJElflDTC9s9sv6+b67tJ0nHp9XGSbkzrOj4ixkTEoWncDZIOkjZ2P91S0qJurhMAAAAACq+ap32ujIjLI+L9knaUNFUVntJZyvaVkv4qaQ/bz9o+QdK5kt5re6ak96b37blY0m7pZyKuknRce10+AQAAAADVqeZH3jeKiCWSfpH+Opv22Aqj3lPFvC9J+nhXYgMAAAAAVFbVTz0AAAAAAFobyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyR8AAAAAFADJHwAAAAAUAMkfAAAAABQAyR8AAAAAFECfvAOopxUrVuiWW27JO4yaPPjgg9qwYUPeYdTkTStW6AMjRuQdRk2emT+/5T9LU6ZM0be+9S3ZzjuUwlu9enXLf56efPLJli/DonXr9O7rrss7jJosWrRIk++9VwsWLsw7lJqcfvrpOuWUU/IOoyb77LNP3iHUxbJly1p6337iiSe03Xbb6bHHHss7lJo8uNe/ym9t7c/UY/fdp1v23TfvMGpy00W/1Dd23jnvMBqqVyV//fr102GHHZZ3GDXZbLPNdMghh+QdRk1mfuc7GjNtWt5h1GSrESNa/rM0depUPfvss3mHAUl9+/Zt+c/T4sWLW74Mc+bM0eFHHZV3GDWZM2eO5s+fr3HjxuUdSk1OP/10PfPMM3mHUZP9998/7xDqYsCAAS29bw8ePFjDhw/XqFGj8g6lJhGhw97//rzDqMmK1Wt12BGt3cbeeMsf9Mwtt+YdRk369u3b4Xi6fQIAAABAAZD8AQAAAEABkPwBAAAAQAGQ/AEAAABAAZD8AQAAAEABkPwBAAAAQAGQ/AEAAABAAZD8AQAAAEABkPwBAAAAQAGQ/AEAAABAAZD8AQAAAEABkPwBAAAAQAGQ/AEAAABAATQs+bN9se0FtmeUDBts+07bM9P/QRXmvdr21PQ3y/bURsUJAAAAAEXQyCt/EyVNKBt2hqS7ImK0pLvS+01ExEcjYkxEjJF0raTrGhgnAAAAAPR6DUv+ImKypCVlg4+QdGl6famkIztahm1L+oikK+sdHwAAAAAUSU/f8zciIuZJUvo/vJPpD5A0PyJmNjwyAAAAAOjFmv2BL8eqk6t+tk+0/YDtBxYvXtxDYQFAx0rbpmXLluUdDgBsVNo+LVy4MO9wAPSgnk7+5tseKUnp/4L0+pL0cJdb2ya03UfSByVd3dECI+LCiBgbEWOHDBnSwNABoHqlbdOAAQPyDgcANiptn4YNG5Z3OAB6UJ8eXt9Nko6TdG76f6MkRcTx7Ux7sKTHIuLZngsPAAAAAHqnRv7Uw5WS/ippD9vP2j5BWdL3XtszJb03va/kGPGgFwAAAACoi4Zd+YuIYyuMek+V83+qftEAAAAAQLE1+wNfAAAAAAB1QPIHAAAAAAVA8gcAAAAABUDyBwAAAAAFQPIHAAAAAAVA8gcAAAAABUDyBwAAAAAFQPIHAAAAAAVA8gcAAAAABUDyBwAAAAAFQPIHAAAAAAVA8gcAAAAABUDyBwAAAAAFQPIHAAAAAAXgiMg7hrqxvVDS7AavZqikRQ1eR6NRhubQG8ogNb4cu0TEsAYuv+Fom6pGGZpHbyhHT5SB9qlzveGzJPWOclCG5pBr29Srkr+eYPuBiBibdxy1oAzNoTeUQeo95Wh1vaEeKEPz6A3l6A1l6A16Sz30hnJQhuaQdxno9gkAAAAABUDyBwAAAAAFQPLXdRfmHUAdUIbm0BvKIPWecrS63lAPlKF59IZy9IYy9Aa9pR56QzkoQ3PItQzc8wcAAAAABcCVPwAAAAAoAJI/SbYvtr3A9oySYYNt32l7Zvo/qMK837I9zfZU23fY3r7nIt8klm6Xo2T6L9kO20MbH3G766+lLs62/Vyqi6m2D+25yF8VR031YPsU24/b/oft83om6k1iqKUeri6pg1m2p/ZY4L1Qb2ifaJtom+qJ9qk50DZtnD7XtinFQPvUBO1Tq7RNJH+ZiZImlA07Q9JdETFa0l3pfXvOj4i9I2KMpJslfb1RQVZhorpfDtneSdJ7Jc1pVIBVmKgayiDpBxExJv3d2qAYOzNR3SyD7fGSjpC0d0TsJel7DYyzIxPVzTJExEfb6kDStZKua2CcRTBRrd8+TRRtE21T/UwU7VMzmCjapmZomyTap2ZpnyaqFdqmiOAvu+9xV0kzSt4/Lmlkej1S0uNVLONMST9r1XJI+p2kN0uaJWloq5VB0tmSvpT3Z6nGMlwj6eC846/1s5SmsaRnJI3Ouyyt/tcb2ifaJtqmZihHyfS0T01QD2k62qYcy0H71BxlKJm+4W0TV/4qGxER8yQp/R9eaULb59h+RtLHlO+Vv/ZUVQ7bh0t6LiIe7sngqlR1XUg6OXUlubizrho9rNoy7C7pANtTbN9je1yPRdi5rtSDJB0gaX5EzGx4ZMXTG9on2qbm0BvaJon2qVnQNjUP2qfm0HRtE8lfHUTEVyNiJ0mXSzo573i6yvY2kr6q5mp8u+NnkkZJGiNpnqTv5xpN9/SRNEjSfpK+LOka2843pG47VtKVeQdRdK3cPtE2NZXe1DZJtE+5o21qGrRPzaXhbRPJX2XzbY+UpPR/QXp9SboZs70+0VdI+lAPxliNasoxStJrJT1se5akHSU9ZPs1OcVcrqq6iIj5EbE+IjZIukjSvrlFvKlqP0/PSrouMvdJ2iApt5vIy1S9T9juI+mDkq7OJdLerze0T7RNzaE3tE0S7VOzoG1qHrRPzaHp2qY+jVx4i7tJ0nGSzk3/b5SkiDi+dCLbo0suzR4u6bGeDLIKVZVDJZehU0M2NiIW9VCMnam2Lka2XVqXdJSkGWoe1dbDDZIOkjTJ9u6StpTUUvWQHCzpsYh4tufCK5Te0D7RNjWH3tA2SbRPzYK2qXnQPjWH5mubGnUzYSv9Kbu8Ok/Sy8rOHpwgaYiyp/LMTP8HV5j3WmU7yjRJv5e0QyuWo2w5s5TTjcs11sVlkqanurhJ6QbbFivDlpJ+kz5TD0k6qNXKkOafKOmkPGLvbX+9oX2ibaJtapZypPlpn/L/PNE2NUk5aJ+aowxp/h5pm5xWBgAAAADoxbjnDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKgOQPAAAAAAqA5A8AAAAACoDkDwAAAAAKoE/eAeRlwoQJsWjRovQuXhkRpVNFhdeSosK4KJuu4vA85+9gWXWdv57bNe/5K2yXDqfJs146Kld35m9/9orDq52umvk7mqZJ56/4cal2ukrL7WBAxY94V9fd0XI7WEee6+9ou+Sy/irUuus0almNXG5vKUve9d2dediuzbmsvNef9+etUctthfV3J0ZJt0fEhO7N+orCJn+LFi3SAw88kL3ZsO6VEdW87s48zTQ/sTRm/l4dy/qS1+ra6+7M06hl1TmWKHm/YUPXhpePqzRdpWXVuv7yZXV1/fUsS0fL6k5Z8qyX8qSydNHRxeEdTdfTy6p2PR2tv6vzNGtZOipXq5Wl3nVcz89rkctSz89Yd2KhLJ2vv/zz1hNlkTRUdUC3TwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKACSPwAAAAAoAJI/AAAAACgAkj8AAAAAKABHRN4x5ML2DElr8o4DmxgqaVHeQWAT1Etzol6aD3XSnKiX5kS9NCfqpTltHRFvrHUhfeoRSYtaExFj8w4Cr2b7Aeql+VAvzYl6aT7USXOiXpoT9dKcqJfmZPuBeiyHbp8AAAAAUAAkfwAAAABQAEVO/i7MOwC0i3ppTtRLc6Jemg910pyol+ZEvTQn6qU51aVeCvvAFwAAAAAokiJf+QMAAACAwuh1yZ/tCbYft/2k7TPaGW/bF6Tx02zvU+286L4q6uVjqT6m2f6L7TeXjJtle7rtqfV60hEyVdTLgbaXpW0/1fbXq50X3VdFvXy5pE5m2F5ve3Aax/7SALYvtr0g/UxQe+M5tuSginrh2JKDKuqFY0sOqqgXji05sL2T7bttP2r7H7Y/38409TvGRESv+ZO0uaSnJO0maUtJD0vas2yaQyX9QZIl7SdpSrXz8tfQetlf0qD0+pC2eknvZ0kamnc5ettflfVyoKSbuzMvf42rl7LpPyDpjyXv2V8aUy/vkrSPpBkVxnNsac564djSnPXCsaUJ66VsWo4tPVcvIyXtk173k/REI/OX3nblb19JT0bE0xHxkqSrJB1RNs0Rkn4dmb9JGmh7ZJXzons63bYR8ZeIeCG9/ZukHXs4xiKq5TPP/tI4Xd22x0q6skciK7CImCxpSQeTcGzJQWf1wrElH1XsL5WwvzRQF+uFY0sPiYh5EfFQer1C0qOSdiibrG7HmN6W/O0g6ZmS989q041XaZpq5kX3dHXbnqDs7EabkHSH7Qdtn9iA+Iqq2np5u+2Hbf/B9l5dnBddV/W2tb2NpAmSri0ZzP6SD44tzY9jS3Ph2NKkOLbkx/aukt4iaUrZqLodY/rUHGVzcTvDyh9nWmmaauZF91S9bW2PV3aAfmfJ4HdExFzbwyXdafuxdPYKtammXh6StEtEvGj7UEk3SBpd5bzonq5s2w9I+nNElJ7JZX/JB8eWJsaxpelwbGluHFtyYHs7ZQn3FyJiefnodmbp1jGmt135e1bSTiXvd5Q0t8ppqpkX3VPVtrW9t6RfSjoiIha3DY+Iuen/AknXK7vEjdp1Wi8RsTwiXkyvb5W0he2h1cyLbuvKtj1GZd1y2F9yw7GlSXFsaT4cW5oex5YeZnsLZYnf5RFxXTuT1O0Y09uSv/sljbb9WttbKvvw3lQ2zU2SPpmemrOfpGURMa/KedE9nW5b2ztLuk7SJyLiiZLh29ru1/Za0vsktfuUKnRZNfXyGttOr/dV1mYsrmZedFtV29b2AEnvlnRjyTD2l/xwbGlCHFuaE8eW5sWxpeelfeFXkh6NiP+pMFndjjG9qttnRKyzfbKk25U9/ebiiPiH7ZPS+J9LulXZE3OelLRK0vEdzZtDMXqdKuvl65KGSPrfdDxYFxFjJY2QdH0a1kfSFRFxWw7F6HWqrJcPS/qc7XWSVks6JrLHS7G/NEiV9SJJR0m6IyJWlszO/tIgtq9U9oTCobaflXSWpC0kji15qqJeOLbkoIp64diSgyrqReLYkod3SPqEpOm2p6Zh/yVpZ6n+xxhn+xoAAAAAoDfrbd0+AQAAAADtIPkDAAAAgAIg+QMAAACAAiD5AwAAAIACIPkDAAAAgAIg+QMANJTtsP39kvdfsn12nZY90faH67GsTtZztO1Hbd9dYfwXba9Jv5HVneX/0vaetUVZX7YH2v73vOMAANQPyR8AoNHWSvqg7aF5B1LK9uZdmPwESf8eEeMrjD9W2Y/tHtWdWCLi3yLike7M20ADJZH8AUAvQvIHAGi0dZIulPTF8hHlV+5sv5j+H2j7HtvX2H7C9rm2P2b7PtvTbY8qWczBtu9N070/zb+57fNt3297mu3Pliz3bttXSJreTjzHpuXPsP3dNOzrkt4p6ee2z29nnlGStpP0NWVJYNvwvVK8U1MMo21va/sW2w+ndXw0TTvJ9tj0+oRUlkm2L7L9k5JtdYHtv9h+um27VbutbA+zfW3aJvfbfkcafrbti9P6nrZ9airCuZJGpfjPtz3S9uT0fobtAzqteQBAU+mTdwAAgEL4qaRpts/rwjxvlvQGSUskPS3plxGxr+3PSzpF0hfSdLtKerekUZLutv06SZ+UtCwixtneStKfbd+Rpt9X0hsj4p+lK7O9vaTvSnqrpBck3WH7yIj4pu2DJH0pIh5oJ85jJV0p6V5Je9geHhELJJ0k6UcRcbntLSVtLulQSXMj4rC0zld1E00x/LekfSStkPRHSQ+XTDJSWSL6ekk3SfpdF7bVjyT9ICL+ZHtnSbeneZSWN15SP0mP2/6ZpDPSdhqTYjtN0u0RcU66arpNO9sCANDEuPIHAGi4iFgu6deSTu1s2hL3R8S8iFgr6SlJbcnbdGUJX5trImJDRMxUlvi8XtL7JH3S9lRJUyQNkTQ6TX9feeKXjJM0KSIWRsQ6SZdLelcVcR4j6aqI2CDpOklHp+F/lfRftk+XtEtErE6xH2z7u7YPiIhlZcvaV9I9EbEkIl6W9Nuy8Teksj4iaUTJ8Gq21cGSfpK2yU2S+tvul8bdEhFrI2KRpAVly964DknHp/s13xQRK6rYNgCAJkLyBwDoKT9Udu/ctiXD1ikdi2xb0pYl49aWvN5Q8n6DXt1zJcrWE5Is6ZSIGJP+XhsRbQnRygrxucpyvDKDvbeypPJO27OUJYLHSlJEXCHpcEmrJd1u+6CIeELZlcXpkr6TupR2JYbSbeIKwyttq80kvb1km+xQksCVzr9e7fQMiojJypLh5yRdZvuTncQKAGgyJH8AgB4REUskXaMsAWwzS1kyJElHSNqiG4s+2vZm6d623SQ9rqxL4+dsbyFJtne3vW1HC1F2hfDdtoembo3HSrqnk3mOlXR2ROya/raXtIPtXWzvJunpiLhA2ZW2vVO3zlUR8RtJ31PWvbPUfSmGQbb7SPpQtRuhCndIOrntje0xnUy/Qlk30Lbpd5G0ICIukvQrbRo7AKDJcc8fAKAnfV8lCYikiyTdaPs+SXep8lW5jjyuLEkbIemkiFhj+5fKujs+lK4oLpR0ZEcLiYh5ts+UdLeyq2q3RsSNnaz7GEmHlA27Pg3fTNLHbb8s6XlJ31TWtfR82xskvSzpc2UxPGf7/ylLROdKekRSedfQ7jpV0k9tT1N2/J+s7L7EdkXEYtt/tj1D0h8kzZD05VSeF5XdVwkAaCGOKO8tAwAA8mJ7u4h4MV35u17SxRFxfd5xAQBaH90+AQBoLmenh7LMkPRPSTfkGg0AoNfgyh8AAAAAFABX/gAAAACgAEj+AAAAAKAASP4AAAAAoABI/gAAAACgAEj+AAAAAKAASP4AAAAAoAD+P5yzLSp2Q9CwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharey = True, figsize = [15, 5])\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "pcm = axs[0].pcolor(CMSC, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[0].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_yticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_ylabel(\"Actual Condition\")\n",
    "axs[0].set_xlabel(\"Predicted Condition\")\n",
    "axs[0].xaxis.set_label_position('top') \n",
    "axs[0].set_title('Raw Data');\n",
    "\n",
    "axs[1].pcolor(CMFT, edgecolors = 'k', cmap = 'gist_heat_r');\n",
    "plt.gca().invert_yaxis()\n",
    "axs[1].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[1].set_title('Fourier Transform');\n",
    "axs[1].set_xlabel(\"Predicted Condition\")\n",
    "axs[1].xaxis.set_label_position('top')\n",
    "\n",
    "axs[2].pcolor(CMHT, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[2].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels);\n",
    "axs[2].set_title('Walsh Hadamard Transform');\n",
    "axs[2].set_xlabel(\"Predicted Condition\")\n",
    "axs[2].xaxis.set_label_position('top')\n",
    "\n",
    "fig.colorbar(pcm, ax = axs[:], location = 'bottom', label = 'Number of Assignments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d30a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
