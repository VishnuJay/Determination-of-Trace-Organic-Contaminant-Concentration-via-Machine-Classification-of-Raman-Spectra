{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5953c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import KeyFunctions as me\n",
    "import tensorflow as tf\n",
    "RandState = 92\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b201c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Full Triclosan Dataset\n",
    "df, labels = me.ConstructCombinedChlorDataset()\n",
    "\n",
    "[train, test] = train_test_split(df, random_state = RandState, shuffle = True, train_size = 0.9)\n",
    "\n",
    "y_tn = train.index\n",
    "y_tt = test.index\n",
    "X_tt = test.to_numpy()\n",
    "X_tn = train.to_numpy()\n",
    "\n",
    "#Augment Data to 4000 Spectra\n",
    "X_tnAu, y_tnAu = me.AugmentData(X_tn, y_tn, 4000, df.columns.to_numpy(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdef8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Training Parameters\n",
    "verbose = 1\n",
    "epochsvec = [5, 20, 50]\n",
    "batch_sizevec = [10, 50, 100]\n",
    "epochs = epochsvec[1]\n",
    "batch_size = batch_sizevec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479b91a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 618, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 618, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnS = scaler.transform(X_tnAu)\n",
    "X_ttS = scaler.transform(X_tt)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnS.reshape(X_tnS.shape[0], X_tnS.shape[1], 1)\n",
    "X_tt_p = X_ttS.reshape(X_ttS.shape[0], X_ttS.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)\n",
    "\n",
    "ytruth = tf.argmax(input = y_ttT, axis = 1).numpy()\n",
    "ytruth = encoder.inverse_transform(ytruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374fecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "72/72 [==============================] - 6s 73ms/step - loss: 0.1242 - accuracy: 0.9564 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "72/72 [==============================] - 4s 61ms/step - loss: 3.2508e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "72/72 [==============================] - 4s 61ms/step - loss: 1.1665e-04 - accuracy: 1.0000 - val_loss: 7.9324e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "72/72 [==============================] - 5s 73ms/step - loss: 6.9474e-05 - accuracy: 1.0000 - val_loss: 5.6363e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "72/72 [==============================] - 5s 63ms/step - loss: 3.6135e-05 - accuracy: 1.0000 - val_loss: 4.1842e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "72/72 [==============================] - 4s 60ms/step - loss: 2.4984e-05 - accuracy: 1.0000 - val_loss: 3.5507e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "72/72 [==============================] - 5s 67ms/step - loss: 1.8166e-05 - accuracy: 1.0000 - val_loss: 2.5547e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "72/72 [==============================] - 5s 69ms/step - loss: 1.2060e-05 - accuracy: 1.0000 - val_loss: 2.0152e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "72/72 [==============================] - 4s 60ms/step - loss: 9.4990e-06 - accuracy: 1.0000 - val_loss: 1.7781e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "72/72 [==============================] - 4s 60ms/step - loss: 6.9684e-06 - accuracy: 1.0000 - val_loss: 1.4679e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "72/72 [==============================] - 5s 73ms/step - loss: 5.9998e-06 - accuracy: 1.0000 - val_loss: 1.2412e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "72/72 [==============================] - 4s 62ms/step - loss: 5.2429e-06 - accuracy: 1.0000 - val_loss: 1.0443e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "72/72 [==============================] - 4s 60ms/step - loss: 3.6332e-06 - accuracy: 1.0000 - val_loss: 9.1004e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "72/72 [==============================] - 5s 67ms/step - loss: 3.3256e-06 - accuracy: 1.0000 - val_loss: 8.4837e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "72/72 [==============================] - 5s 68ms/step - loss: 2.6968e-06 - accuracy: 1.0000 - val_loss: 7.7201e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "72/72 [==============================] - 4s 61ms/step - loss: 2.1251e-06 - accuracy: 1.0000 - val_loss: 6.9520e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "72/72 [==============================] - 4s 62ms/step - loss: 1.9516e-06 - accuracy: 1.0000 - val_loss: 6.1116e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "72/72 [==============================] - 5s 72ms/step - loss: 1.8313e-06 - accuracy: 1.0000 - val_loss: 5.7318e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "72/72 [==============================] - 4s 60ms/step - loss: 1.6216e-06 - accuracy: 1.0000 - val_loss: 5.9084e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "72/72 [==============================] - 4s 60ms/step - loss: 1.3380e-06 - accuracy: 1.0000 - val_loss: 4.5488e-05 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.6080 - accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6000000238418579"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 3,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_split = 0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, SCaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(SCaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "SCypred = model.predict(X_ttT)\n",
    "SCypred = tf.argmax(input = SCypred, axis = 1).numpy()\n",
    "SCypred = encoder.inverse_transform(SCypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Convolution Feature Maps\n",
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec = list()\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "SCfeature_maps = convlayer.predict(spectra)\n",
    "display(SCfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnS= scaler.transform(X_tnAu)\n",
    "X_ttS = scaler.transform(X_tt)\n",
    "\n",
    "#Apply Fourier Transform to Training and Testing Data\n",
    "X_tnf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tnS)\n",
    "X_ttf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_ttS)\n",
    "\n",
    "#Combine Real and Imaginary Part of FT in form [real, imaginary]\n",
    "X_tnf = np.append(X_tnf.real, X_tnf.imag, axis = 1)\n",
    "X_ttf = np.append(X_ttf.real, X_ttf.imag, axis = 1)\n",
    "X_tnf= X_tnf.astype('float32')\n",
    "X_ttf= X_ttf.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_tnf)\n",
    "#X_tnf = scaler.transform(X_tnf)\n",
    "#X_ttf = scaler.transform(X_ttf)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnf.reshape(X_tnf.shape[0], X_tnf.shape[1], 1)\n",
    "X_tt_p = X_ttf.reshape(X_ttf.shape[0], X_ttf.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e552db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 3,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.2, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, FTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(FTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54173aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "FTypred = model.predict(X_ttT)\n",
    "FTypred = tf.argmax(input = FTypred, axis = 1).numpy()\n",
    "FTypred = encoder.inverse_transform(FTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0580f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "convlayer.summary()\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "FTfeature_maps = convlayer.predict(spectra)\n",
    "display(FTfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756a0320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 1024, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 1024, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply Welsh-Hadamard Transform to Training and Testing Data\n",
    "from sympy.discrete.transforms import fwht, ifwht\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_tnAu)\n",
    "#X_tnAu = scaler.transform(X_tnAu)\n",
    "#X_tt = scaler.transform(X_tt)\n",
    "\n",
    "X_tnh = np.apply_along_axis(fwht, axis=1, arr=X_tnAu)\n",
    "X_tth = np.apply_along_axis(fwht, axis=1, arr=X_tt)\n",
    "X_tnh = X_tnh.astype('float32')\n",
    "X_tth = X_tth.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnh)\n",
    "X_tnh = scaler.transform(X_tnh)\n",
    "X_tth = scaler.transform(X_tth)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnh.reshape(X_tnh.shape[0], X_tnh.shape[1], 1)\n",
    "X_tt_p = X_tth.reshape(X_tth.shape[0], X_tth.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e029d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "72/72 [==============================] - 8s 103ms/step - loss: 0.3363 - accuracy: 0.8989 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "72/72 [==============================] - 8s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "72/72 [==============================] - 7s 98ms/step - loss: 6.7148e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "72/72 [==============================] - 8s 110ms/step - loss: 3.6129e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "72/72 [==============================] - 7s 100ms/step - loss: 2.5231e-04 - accuracy: 1.0000 - val_loss: 7.6174e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "72/72 [==============================] - 8s 114ms/step - loss: 1.8237e-04 - accuracy: 1.0000 - val_loss: 5.6492e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 1.3473e-04 - accuracy: 1.0000 - val_loss: 5.6061e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "72/72 [==============================] - 8s 113ms/step - loss: 9.8767e-05 - accuracy: 1.0000 - val_loss: 4.5277e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 7.9205e-05 - accuracy: 1.0000 - val_loss: 3.6445e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "72/72 [==============================] - 8s 113ms/step - loss: 6.1507e-05 - accuracy: 1.0000 - val_loss: 2.8766e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "72/72 [==============================] - 7s 97ms/step - loss: 5.4866e-05 - accuracy: 1.0000 - val_loss: 2.7842e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "72/72 [==============================] - 8s 112ms/step - loss: 4.3037e-05 - accuracy: 1.0000 - val_loss: 2.4675e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "72/72 [==============================] - 7s 103ms/step - loss: 3.6913e-05 - accuracy: 1.0000 - val_loss: 1.9774e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "72/72 [==============================] - 8s 113ms/step - loss: 3.5452e-05 - accuracy: 1.0000 - val_loss: 1.8344e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "72/72 [==============================] - 7s 99ms/step - loss: 2.6320e-05 - accuracy: 1.0000 - val_loss: 1.6867e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "72/72 [==============================] - 8s 110ms/step - loss: 2.6191e-05 - accuracy: 1.0000 - val_loss: 1.4159e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "72/72 [==============================] - 7s 102ms/step - loss: 2.1445e-05 - accuracy: 1.0000 - val_loss: 1.2639e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "72/72 [==============================] - 8s 107ms/step - loss: 1.8871e-05 - accuracy: 1.0000 - val_loss: 1.1986e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "72/72 [==============================] - 8s 105ms/step - loss: 1.8139e-05 - accuracy: 1.0000 - val_loss: 1.0929e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "72/72 [==============================] - 8s 105ms/step - loss: 1.5965e-05 - accuracy: 1.0000 - val_loss: 9.5243e-05 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 1.7370 - accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.800000011920929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = 'min',\\\n",
    "                                           patience = 3, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, HTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "HTypred = model.predict(X_ttT)\n",
    "HTypred = tf.argmax(input = HTypred, axis = 1).numpy()\n",
    "HTypred = encoder.inverse_transform(HTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ca327",
   "metadata": {},
   "outputs": [],
   "source": [
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "convlayer.summary()\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "HTfeature_maps = convlayer.predict(spectra)\n",
    "display(HTfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "display(SCypred)\n",
    "CMSC = confusion_matrix(ytruth, SCypred, labels = labels)\n",
    "CMFT = confusion_matrix(ytruth, FTypred, labels = labels)\n",
    "CMHT = confusion_matrix(ytruth, HTypred, labels = labels)\n",
    "display(SCaccuracy, FTaccuracy, HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharey = True, figsize = [15, 5])\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "pcm = axs[0].pcolor(CMSC, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[0].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_yticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_ylabel(\"Actual Condition\")\n",
    "axs[0].set_xlabel(\"Predicted Condition\")\n",
    "axs[0].xaxis.set_label_position('top') \n",
    "axs[0].set_title('Raw Data');\n",
    "\n",
    "axs[1].pcolor(CMFT, edgecolors = 'k', cmap = 'gist_heat_r');\n",
    "plt.gca().invert_yaxis()\n",
    "axs[1].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[1].set_title('Fourier Transform');\n",
    "axs[1].set_xlabel(\"Predicted Condition\")\n",
    "axs[1].xaxis.set_label_position('top')\n",
    "\n",
    "axs[2].pcolor(CMHT, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[2].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels);\n",
    "axs[2].set_title('Walsh Hadamard Transform');\n",
    "axs[2].set_xlabel(\"Predicted Condition\")\n",
    "axs[2].xaxis.set_label_position('top')\n",
    "\n",
    "fig.colorbar(pcm, ax = axs[:], location = 'bottom', label = 'Number of Assignments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d30a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
