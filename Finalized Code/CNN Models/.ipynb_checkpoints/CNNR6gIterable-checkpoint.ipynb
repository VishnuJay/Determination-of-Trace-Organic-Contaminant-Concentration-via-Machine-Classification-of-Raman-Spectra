{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5953c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import KeyFunctions as me\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sympy.discrete.transforms import fwht, ifwht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0393567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145 375 189 233 225]\n"
     ]
    }
   ],
   "source": [
    "RSvec = np.random.randint(1, 500, 5)\n",
    "n = 4\n",
    "print(RSvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42410f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandState = RSvec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e771cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for RandState in RSvec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22b201c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  145\n"
     ]
    }
   ],
   "source": [
    "#Import Full R6g Dataset\n",
    "print('Random State: ', RandState)\n",
    "df, labels = me.ConstructCombinedDataset()\n",
    "\n",
    "[train, test] = train_test_split(df, random_state = RandState, shuffle = True, train_size = 0.9, stratify = df.index)\n",
    "\n",
    "y_tn = train.index\n",
    "y_tt = test.index\n",
    "X_tt = test.to_numpy()\n",
    "X_tn = train.to_numpy()\n",
    "\n",
    "#Augment Data to 4000 Spectra\n",
    "X_tnAu, y_tnAu = me.AugmentData(X_tn, y_tn, 4000, df.columns.to_numpy(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdef8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Training Parameters\n",
    "verbose = 0\n",
    "epochsvec = [5, 20, 50]\n",
    "batch_sizevec = [10, 50, 100]\n",
    "epochs = epochsvec[1]\n",
    "batch_size = batch_sizevec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479b91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnS = scaler.transform(X_tnAu)\n",
    "X_ttS = scaler.transform(X_tt)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnS.reshape(X_tnS.shape[0], X_tnS.shape[1], 1)\n",
    "X_tt_p = X_ttS.reshape(X_ttS.shape[0], X_ttS.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "ytruth = tf.argmax(input = y_ttT, axis = 1).numpy()\n",
    "ytruth = encoder.inverse_transform(ytruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374fecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "72/72 - 4s - loss: 1.1405 - accuracy: 0.5353 - val_loss: 0.9486 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "72/72 - 3s - loss: 0.7562 - accuracy: 0.7011 - val_loss: 0.7808 - val_accuracy: 0.7650\n",
      "Epoch 3/20\n",
      "72/72 - 3s - loss: 0.6751 - accuracy: 0.7472 - val_loss: 0.6864 - val_accuracy: 0.7775\n",
      "Epoch 4/20\n",
      "72/72 - 3s - loss: 0.6019 - accuracy: 0.7683 - val_loss: 0.6334 - val_accuracy: 0.7450\n",
      "Epoch 5/20\n",
      "72/72 - 4s - loss: 0.5500 - accuracy: 0.7875 - val_loss: 0.5595 - val_accuracy: 0.7775\n",
      "Epoch 6/20\n",
      "72/72 - 4s - loss: 0.4978 - accuracy: 0.8083 - val_loss: 0.5363 - val_accuracy: 0.8625\n",
      "Epoch 7/20\n",
      "72/72 - 3s - loss: 0.4444 - accuracy: 0.8306 - val_loss: 0.4595 - val_accuracy: 0.8400\n",
      "Epoch 8/20\n",
      "72/72 - 3s - loss: 0.4616 - accuracy: 0.8297 - val_loss: 0.6079 - val_accuracy: 0.7750\n",
      "Epoch 9/20\n",
      "72/72 - 4s - loss: 0.4234 - accuracy: 0.8364 - val_loss: 0.4249 - val_accuracy: 0.8300\n",
      "Epoch 10/20\n",
      "72/72 - 4s - loss: 0.3833 - accuracy: 0.8425 - val_loss: 0.4455 - val_accuracy: 0.8675\n",
      "Epoch 11/20\n",
      "72/72 - 4s - loss: 0.3608 - accuracy: 0.8500 - val_loss: 0.3920 - val_accuracy: 0.8675\n",
      "Epoch 12/20\n",
      "72/72 - 3s - loss: 0.3268 - accuracy: 0.8669 - val_loss: 0.3814 - val_accuracy: 0.8450\n",
      "Epoch 13/20\n",
      "72/72 - 3s - loss: 0.3347 - accuracy: 0.8681 - val_loss: 0.3813 - val_accuracy: 0.8675\n",
      "Epoch 14/20\n",
      "72/72 - 3s - loss: 0.3107 - accuracy: 0.8772 - val_loss: 0.3509 - val_accuracy: 0.8725\n",
      "Epoch 15/20\n",
      "72/72 - 4s - loss: 0.3209 - accuracy: 0.8706 - val_loss: 0.3477 - val_accuracy: 0.8675\n",
      "Epoch 16/20\n",
      "72/72 - 3s - loss: 0.2788 - accuracy: 0.8889 - val_loss: 0.3350 - val_accuracy: 0.8525\n",
      "Epoch 17/20\n",
      "72/72 - 3s - loss: 0.2588 - accuracy: 0.8997 - val_loss: 0.2763 - val_accuracy: 0.8925\n",
      "Epoch 18/20\n",
      "72/72 - 3s - loss: 0.2484 - accuracy: 0.9011 - val_loss: 0.2918 - val_accuracy: 0.9325\n",
      "Epoch 19/20\n",
      "72/72 - 3s - loss: 0.2385 - accuracy: 0.9019 - val_loss: 0.2646 - val_accuracy: 0.8875\n",
      "Epoch 20/20\n",
      "72/72 - 4s - loss: 0.2657 - accuracy: 0.8919 - val_loss: 0.2759 - val_accuracy: 0.8900\n",
      "1/1 - 0s - loss: 0.6350 - accuracy: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5833333134651184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "history_sc = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_split = 0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "SCloss, SCaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ccf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "SCypred = model.predict(X_ttT)\n",
    "SCypred = tf.argmax(input = SCypred, axis = 1).numpy()\n",
    "SCypred = encoder.inverse_transform(SCypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SCALED')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(SCaccuracy, SCloss, history_sc.history['accuracy'][-1], history_sc.history['loss'][-1], history_sc.history['val_accuracy'][-1], history_sc.history['val_loss'][-1], me.Scorer(ytruth, SCypred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d53fd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Fourier Transform to Training and Testing Data\n",
    "X_tnf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tnAu)\n",
    "X_ttf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tt)\n",
    "\n",
    "#Combine Real and Imaginary Part of FT in form [real, imaginary]\n",
    "X_tnf = np.append(X_tnf.real, X_tnf.imag, axis = 1)\n",
    "X_ttf = np.append(X_ttf.real, X_ttf.imag, axis = 1)\n",
    "X_tnf= X_tnf.astype('float32')\n",
    "X_ttf= X_ttf.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnf)\n",
    "X_tnf = scaler.transform(X_tnf)\n",
    "X_ttf = scaler.transform(X_ttf)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnf.reshape(X_tnf.shape[0], X_tnf.shape[1], 1)\n",
    "X_tt_p = X_ttf.reshape(X_ttf.shape[0], X_ttf.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18e552db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "history_ft = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "FTloss, FTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54173aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "FTypred = model.predict(X_ttT)\n",
    "FTypred = tf.argmax(input = FTypred, axis = 1).numpy()\n",
    "FTypred = encoder.inverse_transform(FTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed9882fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourier\n",
      "Test Acc Test Loss Train Acc Train Loss Val_Acc Val_Loss Regression Error\n",
      "0.75 0.6742073893547058 0.9963889122009277 0.0113621074706316 0.9725000262260437 0.058043286204338074 10.222222222222221\n"
     ]
    }
   ],
   "source": [
    "print('Fourier')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(FTaccuracy, FTloss, history_ft.history['accuracy'][-1], history_ft.history['loss'][-1], history_ft.history['val_accuracy'][-1], history_ft.history['val_loss'][-1], me.Scorer(ytruth, FTypred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Welsh-Hadamard Transform to Training and Testing Data\n",
    "from sympy.discrete.transforms import fwht, ifwht\n",
    "X_tnh = np.apply_along_axis(fwht, axis=1, arr=X_tnAu)\n",
    "X_tth = np.apply_along_axis(fwht, axis=1, arr=X_tt)\n",
    "X_tnh = X_tnh.astype('float32')\n",
    "X_tth = X_tth.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnh)\n",
    "X_tnh = scaler.transform(X_tnh)\n",
    "X_tth = scaler.transform(X_tth)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnh.reshape(X_tnh.shape[0], X_tnh.shape[1], 1)\n",
    "X_tt_p = X_tth.reshape(X_tth.shape[0], X_tth.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e029d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = 'min',\\\n",
    "                                           patience = 2, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "history_ht = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "HTloss, HTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "HTypred = model.predict(X_ttT)\n",
    "HTypred = tf.argmax(input = HTypred, axis = 1).numpy()\n",
    "HTypred = encoder.inverse_transform(HTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396047ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('HADAMARD')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(HTaccuracy, HTloss, history_ht.history['accuracy'][-1], history_ht.history['loss'][-1], history_ht.history['val_accuracy'][-1], history_ht.history['val_loss'][-1], me.Scorer(ytruth, HTypred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ea403",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
