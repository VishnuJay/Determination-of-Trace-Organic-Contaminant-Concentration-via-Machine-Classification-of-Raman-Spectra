{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import KeyFunctions as me\n",
    "import tensorflow as tf\n",
    "import os, sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sympy.discrete.transforms import fwht, ifwht\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from contextlib import closing\n",
    "\n",
    "RandState = 117\n",
    "\n",
    "f = open('R6gResult.txt', 'w')\n",
    "sys.stdout = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, labels = me.ConstructCombinedDataset()\n",
    "\n",
    "KFold = StratifiedKFold(shuffle = True, random_state = RandState)\n",
    "Folds = KFold.split(df, df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCtnacc = []\n",
    "SCtnloss = []\n",
    "SCttacc = []\n",
    "SCttloss = []\n",
    "SCvlacc = []\n",
    "SCvlloss = []\n",
    "SCregerr= []\n",
    "\n",
    "FTtnacc = []\n",
    "FTtnloss = []\n",
    "FTttacc = []\n",
    "FTttloss = []\n",
    "FTvlacc = []\n",
    "FTvlloss = []\n",
    "FTregerr= []\n",
    "\n",
    "HTtnacc = []\n",
    "HTtnloss = []\n",
    "HTttacc = []\n",
    "HTttloss = []\n",
    "HTvlacc = []\n",
    "HTvlloss = []\n",
    "HTregerr= []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Full R6g Dataset\n",
    "for i, (train_index, test_index) in enumerate(Folds):\n",
    "\n",
    "    y_tn = df.index[train_index].to_numpy()\n",
    "    y_tt = df.index[test_index].to_numpy()\n",
    "    X_tt = df.iloc[test_index, :].to_numpy()\n",
    "    X_tn = df.iloc[train_index, :].to_numpy()\n",
    "\n",
    "    #Augment Data to 4000 Spectra\n",
    "    X_tnAu, y_tnAu = me.AugmentData(X_tn, y_tn, 4000, df.columns.to_numpy(), False)\n",
    "\n",
    "    #Set Training Parameters\n",
    "    verbose = 0\n",
    "    epochsvec = [5, 20, 50]\n",
    "    batch_sizevec = [10, 50, 100]\n",
    "    epochs = epochsvec[1]\n",
    "    batch_size = batch_sizevec[1]\n",
    "    prev_time = time.time()\n",
    "\n",
    "    #Scale X-Data with Training Xs\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tnAu)\n",
    "    X_tnS = scaler.transform(X_tnAu)\n",
    "    X_ttS = scaler.transform(X_tt)\n",
    "\n",
    "    #Encode y-Data with Training ys\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_tnAu)\n",
    "    y_tn_e = encoder.transform(y_tnAu)\n",
    "    y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "    y_tt_e = encoder.transform(y_tt)\n",
    "    y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "\n",
    "    #Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "    X_tn_p = X_tnS.reshape(X_tnS.shape[0], X_tnS.shape[1], 1)\n",
    "    X_tt_p = X_ttS.reshape(X_ttS.shape[0], X_ttS.shape[1], 1)\n",
    "\n",
    "    y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "    y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "    X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "    X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "    ytruth = tf.argmax(input = y_ttT, axis = 1).numpy()\n",
    "    ytruth = encoder.inverse_transform(ytruth)\n",
    "\n",
    "    display(\"--- Processing Time: %s seconds ---\" % (time.time() - prev_time))\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    #Multi-class Classification with Keras\n",
    "    n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "    #Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters = 64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #Implement EarlyStopping\n",
    "    stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = verbose, restore_best_weights = True)\n",
    "\n",
    "    #Fit Model\n",
    "    history_sc = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=0, validation_split = 0.1, callbacks = stopper)\n",
    "\n",
    "    #Evaluate Model\n",
    "    SCloss, SCaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    #Make Prediction\n",
    "    SCypred = model.predict(X_ttT)\n",
    "    SCypred = tf.argmax(input = SCypred, axis = 1).numpy()\n",
    "    SCypred = encoder.inverse_transform(SCypred)\n",
    "\n",
    "    #Display Scaled Results\n",
    "    print('SCALED')\n",
    "    print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "    print(SCaccuracy, SCloss, history_sc.history['accuracy'][-1], history_sc.history['loss'][-1], history_sc.history['val_accuracy'][-1], history_sc.history['val_loss'][-1], me.Scorer(ytruth, SCypred, labels))\n",
    "    \n",
    "    SCtnacc.append(SCaccuracy)\n",
    "    SCtnloss.append(SCloss)\n",
    "    SCttacc.append(history_sc.history['accuracy'][-1])\n",
    "    SCttloss.append(history_sc.history['loss'][-1])\n",
    "    SCvlacc.append(history_sc.history['val_accuracy'][-1])\n",
    "    SCvlloss.append(history_sc.history['val_loss'][-1])\n",
    "    SCregerr.append(me.Scorer(ytruth, SCypred, labels))\n",
    "    \n",
    "    display(\"--- Training Time: %s seconds ---\" % (time.time() - prev_time))\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    ####\n",
    "    #Apply Fourier Transform to Training and Testing Data\n",
    "    X_tnf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tnAu)\n",
    "    X_ttf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tt)\n",
    "\n",
    "    #Combine Real and Imaginary Part of FT in form [real, imaginary]\n",
    "    X_tnf = np.append(X_tnf.real, X_tnf.imag, axis = 1)\n",
    "    X_ttf = np.append(X_ttf.real, X_ttf.imag, axis = 1)\n",
    "    X_tnf= X_tnf.astype('float32')\n",
    "    X_ttf= X_ttf.astype('float32')\n",
    "\n",
    "    #Scale X-Data with Training Xs\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tnf)\n",
    "    X_tnf = scaler.transform(X_tnf)\n",
    "    X_ttf = scaler.transform(X_ttf)\n",
    "\n",
    "    #Encode y-Data with Training ys\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_tnAu)\n",
    "    y_tn_e = encoder.transform(y_tnAu)\n",
    "    y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "    y_tt_e = encoder.transform(y_tt)\n",
    "    y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "    #Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "    X_tn_p = X_tnf.reshape(X_tnf.shape[0], X_tnf.shape[1], 1)\n",
    "    X_tt_p = X_ttf.reshape(X_ttf.shape[0], X_ttf.shape[1], 1)\n",
    "\n",
    "    y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "    y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "    X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "    X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "    display(\"--- Processing Time: %s seconds ---\" % (time.time() - prev_time))\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    #Multi-class Classification with Keras\n",
    "    n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "    #Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #Implement EarlyStopping\n",
    "    stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = verbose, restore_best_weights = True)\n",
    "\n",
    "    #Fit Model\n",
    "    history_ft = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=0,  validation_split=0.1, callbacks = stopper)\n",
    "\n",
    "    #Evaluate Model\n",
    "    FTloss, FTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    #Make Prediction\n",
    "    FTypred = model.predict(X_ttT)\n",
    "    FTypred = tf.argmax(input = FTypred, axis = 1).numpy()\n",
    "    FTypred = encoder.inverse_transform(FTypred)\n",
    "\n",
    "    #Fourier Results\n",
    "    print('FOURIER')\n",
    "    print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "    print(FTaccuracy, FTloss, history_ft.history['accuracy'][-1], history_ft.history['loss'][-1], history_ft.history['val_accuracy'][-1], history_ft.history['val_loss'][-1], me.Scorer(ytruth, FTypred, labels))\n",
    "\n",
    "    FTtnacc.append(FTaccuracy)\n",
    "    FTtnloss.append(FTloss)\n",
    "    FTttacc.append(history_ft.history['accuracy'][-1])\n",
    "    FTttloss.append(history_ft.history['loss'][-1])\n",
    "    FTvlacc.append(history_ft.history['val_accuracy'][-1])\n",
    "    FTvlloss.append(history_ft.history['val_loss'][-1])\n",
    "    FTregerr.append(me.Scorer(ytruth, SCypred, labels))\n",
    "    \n",
    "    display(\"--- Training Time: %s seconds ---\" % (time.time() - prev_time))\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    #####\n",
    "    #Apply Welsh-Hadamard Transform to Training and Testing Data\n",
    "    X_tnh = np.apply_along_axis(fwht, axis=1, arr=X_tnAu)\n",
    "    X_tth = np.apply_along_axis(fwht, axis=1, arr=X_tt)\n",
    "    X_tnh = X_tnh.astype('float32')\n",
    "    X_tth = X_tth.astype('float32')\n",
    "\n",
    "    #Scale X-Data with Training Xs\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tnh)\n",
    "    X_tnh = scaler.transform(X_tnh)\n",
    "    X_tth = scaler.transform(X_tth)\n",
    "\n",
    "    #Encode y-Data with Training ys\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_tnAu)\n",
    "    y_tn_e = encoder.transform(y_tnAu)\n",
    "    y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "    y_tt_e = encoder.transform(y_tt)\n",
    "    y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "    #Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "    X_tn_p = X_tnh.reshape(X_tnh.shape[0], X_tnh.shape[1], 1)\n",
    "    X_tt_p = X_tth.reshape(X_tth.shape[0], X_tth.shape[1], 1)\n",
    "\n",
    "    y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "    y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "    X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "    X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "    display(\"--- Processing Time: %s seconds ---\" % (time.time() - prev_time))\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    #Multi-class Classification with Keras\n",
    "\n",
    "    n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "    #Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #Implement EarlyStopping\n",
    "    stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = 'min',\\\n",
    "                                               patience = 2, verbose = 0, restore_best_weights = True)\n",
    "\n",
    "    #Fit Model\n",
    "    history_ht = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.1, callbacks = stopper)\n",
    "\n",
    "    #Evaluate Model\n",
    "    HTloss, HTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    #Make Prediction\n",
    "    HTypred = model.predict(X_ttT)\n",
    "    HTypred = tf.argmax(input = HTypred, axis = 1).numpy()\n",
    "    HTypred = encoder.inverse_transform(HTypred)\n",
    "\n",
    "    print('HADAMARD')\n",
    "    print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "    print(HTaccuracy, HTloss, history_ht.history['accuracy'][-1], history_ht.history['loss'][-1], history_ht.history['val_accuracy'][-1], history_ht.history['val_loss'][-1], me.Scorer(ytruth, HTypred, labels))\n",
    "    \n",
    "    HTtnacc.append(HTaccuracy)\n",
    "    HTtnloss.append(HTloss)\n",
    "    HTttacc.append(history_ht.history['accuracy'][-1])\n",
    "    HTttloss.append(history_ht.history['loss'][-1])\n",
    "    HTvlacc.append(history_ht.history['val_accuracy'][-1])\n",
    "    HTvlloss.append(history_ht.history['val_loss'][-1])\n",
    "    HTregerr.append(me.Scorer(ytruth, SCypred, labels))\n",
    "    \n",
    "    display(\"--- Training Time: %s seconds ---\" % (time.time() - prev_time))\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    print('----------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect Averages\n",
    "SCtnaccavg = np.mean(SCtnacc)\n",
    "SCtnlossavg = np.mean(SCtnloss)\n",
    "SCttaccavg = np.mean(SCttacc)\n",
    "SCttlossavg = np.mean(SCttloss)\n",
    "SCvlaccavg = np.mean(SCvlacc)\n",
    "SCvllossavg = np.mean(SCvlloss)\n",
    "SCregerravg = np.mean(SCregerr)\n",
    "\n",
    "FTtnaccavg = np.mean(FTtnacc)\n",
    "FTtnlossavg = np.mean(FTtnloss)\n",
    "FTttaccavg = np.mean(FTttacc)\n",
    "FTttlossavg = np.mean(FTttloss)\n",
    "FTvlaccavg = np.mean(FTvlacc)\n",
    "FTvllossavg = np.mean(FTvlloss)\n",
    "FTregerravg = np.mean(FTregerr)\n",
    "\n",
    "HTtnaccavg = np.mean(HTtnacc)\n",
    "HTtnlossavg = np.mean(HTtnloss)\n",
    "HTttaccavg = np.mean(HTttacc)\n",
    "HTttlossavg = np.mean(HTttloss)\n",
    "HTvlaccavg = np.mean(HTvlacc)\n",
    "HTvllossavg = np.mean(HTvlloss)\n",
    "HTregerravg = np.mean(HTregerr)\n",
    "\n",
    "#Collect Standard Deviations\n",
    "SCtnaccstd = np.std(SCtnacc)\n",
    "SCtnlossstd = np.std(SCtnloss)\n",
    "SCttaccstd = np.std(SCttacc)\n",
    "SCttlossstd = np.std(SCttloss)\n",
    "SCvlaccstd = np.std(SCvlacc)\n",
    "SCvllossstd = np.std(SCvlloss)\n",
    "SCregerrstd = np.std(SCregerr)\n",
    "\n",
    "FTtnaccstd = np.std(FTtnacc)\n",
    "FTtnlossstd = np.std(FTtnloss)\n",
    "FTttaccstd = np.std(FTttacc)\n",
    "FTttlossstd = np.std(FTttloss)\n",
    "FTvlaccstd = np.std(FTvlacc)\n",
    "FTvllossstd = np.std(FTvlloss)\n",
    "FTregerrstd = np.std(FTregerr)\n",
    "\n",
    "HTtnaccstd = np.std(HTtnacc)\n",
    "HTtnlossstd = np.std(HTtnloss)\n",
    "HTttaccstd = np.std(HTttacc)\n",
    "HTttlossstd = np.std(HTttloss)\n",
    "HTvlaccstd = np.std(HTvlacc)\n",
    "HTvllossstd = np.std(HTvlloss)\n",
    "HTregerrstd = np.std(HTregerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*********************************************************')\n",
    "print('SCALED AVERAGE')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(SCtnaccavg, SCtnlossavg, SCttaccavg, SCttlossavg, SCvlaccavg, SCvllossavg, SCregerravg)\n",
    "print('SCALED STANDARD DEVIATION')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(SCtnaccstd, SCtnlossstd, SCttaccstd, SCttlossstd, SCvlaccstd, SCvllossstd, SCregerrstd)\n",
    "print('----------------------------')\n",
    "\n",
    "print('FOURIER AVERAGE')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(FTtnaccavg, FTtnlossavg, FTttaccavg, FTttlossavg, FTvlaccavg, FTvllossavg, FTregerravg)\n",
    "print('FOURIER STANDARD DEVIATION')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(FTtnaccstd, FTtnlossstd, FTttaccstd, FTttlossstd, FTvlaccstd, FTvllossstd, FTregerrstd)\n",
    "print('----------------------------')\n",
    "print('HADAMARD AVERAGE')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(HTtnaccavg, HTtnlossavg, HTttaccavg, HTttlossavg, HTvlaccavg, HTvllossavg, HTregerravg)\n",
    "print('HADAMARD STANDARD DEVIATION')\n",
    "print('Test Acc', 'Test Loss', 'Train Acc', 'Train Loss', 'Val_Acc', 'Val_Loss', 'Regression Error')\n",
    "print(HTtnaccstd, HTtnlossstd, HTttaccstd, HTttlossstd, HTvlaccstd, HTvllossstd, HTregerrstd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
