{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5953c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import KeyFunctions as me\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, MaxPooling2D, LSTM, TimeDistributed, Reshape\n",
    "from keras.backend import reshape\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "RandState = 219\n",
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbf017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b201c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Full R6g Dataset\n",
    "df, labels = me.ConstructCombinedDataset()\n",
    "\n",
    "[train, test] = train_test_split(df, random_state = RandState, shuffle = True, train_size = 0.9, stratify = df.index)\n",
    "\n",
    "y_tn = train.index\n",
    "y_tt = test.index\n",
    "X_tt = test.to_numpy()\n",
    "X_tn = train.to_numpy()\n",
    "\n",
    "#Augment Data to 4000 Spectra\n",
    "X_tnAu, y_tnAu = me.AugmentData(X_tn, y_tn, 4000, df.columns.to_numpy(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdef8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Training Parameters\n",
    "verbose = 1\n",
    "epochsvec = [5, 20, 50]\n",
    "batch_sizevec = [10, 50, 100]\n",
    "epochs = epochsvec[1]\n",
    "batch_size = batch_sizevec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnS = scaler.transform(X_tnAu)\n",
    "X_ttS = scaler.transform(X_tt)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnS.reshape(X_tnS.shape[0], X_tnS.shape[1], 1)\n",
    "X_tt_p = X_ttS.reshape(X_ttS.shape[0], X_ttS.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)\n",
    "\n",
    "ytruth = tf.argmax(input = y_ttT, axis = 1).numpy()\n",
    "ytruth = encoder.inverse_transform(ytruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "history_sc = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_split = 0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, SCaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(SCaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "SCypred = model.predict(X_ttT)\n",
    "SCypred = tf.argmax(input = SCypred, axis = 1).numpy()\n",
    "SCypred = encoder.inverse_transform(SCypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Fourier Transform to Training and Testing Data\n",
    "X_tnf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tnAu)\n",
    "X_ttf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tt)\n",
    "\n",
    "#Combine Real and Imaginary Part of FT in form [real, imaginary]\n",
    "X_tnf = np.append(X_tnf.real, X_tnf.imag, axis = 1)\n",
    "X_ttf = np.append(X_ttf.real, X_ttf.imag, axis = 1)\n",
    "X_tnf= X_tnf.astype('float32')\n",
    "X_ttf= X_ttf.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnf)\n",
    "X_tnf = scaler.transform(X_tnf)\n",
    "X_ttf = scaler.transform(X_ttf)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnf.reshape(X_tnf.shape[0], X_tnf.shape[1], 1)\n",
    "X_tt_p = X_ttf.reshape(X_ttf.shape[0], X_ttf.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e552db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "history_ft = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, FTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(FTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54173aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "FTypred = model.predict(X_ttT)\n",
    "FTypred = tf.argmax(input = FTypred, axis = 1).numpy()\n",
    "FTypred = encoder.inverse_transform(FTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756a0320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 512, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([12, 512, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([12, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply Welsh-Hadamard Transform to Training and Testing Data\n",
    "from sympy.discrete.transforms import fwht, ifwht\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnAu = scaler.transform(X_tnAu)\n",
    "X_tt = scaler.transform(X_tt)\n",
    "\n",
    "X_tnh = np.apply_along_axis(fwht, axis=1, arr=X_tnAu)\n",
    "X_tth = np.apply_along_axis(fwht, axis=1, arr=X_tt)\n",
    "X_tnh = X_tnh.astype('float32')\n",
    "X_tth = X_tth.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnh)\n",
    "X_tnh = scaler.transform(X_tnh)\n",
    "X_tth = scaler.transform(X_tth)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnh.reshape(X_tnh.shape[0], X_tnh.shape[1], 1)\n",
    "X_tt_p = X_tth.reshape(X_tth.shape[0], X_tth.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e029d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "72/72 [==============================] - 5s 57ms/step - loss: 0.9541 - accuracy: 0.6225 - val_loss: 0.6856 - val_accuracy: 0.7125\n",
      "Epoch 2/20\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.6724 - accuracy: 0.7314 - val_loss: 0.5787 - val_accuracy: 0.8675\n",
      "Epoch 3/20\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.5110 - accuracy: 0.8119 - val_loss: 0.4555 - val_accuracy: 0.8500\n",
      "Epoch 4/20\n",
      "72/72 [==============================] - 4s 50ms/step - loss: 0.3969 - accuracy: 0.8567 - val_loss: 0.2767 - val_accuracy: 0.9350\n",
      "Epoch 5/20\n",
      "72/72 [==============================] - 4s 58ms/step - loss: 0.2729 - accuracy: 0.9000 - val_loss: 0.2253 - val_accuracy: 0.9025\n",
      "Epoch 6/20\n",
      "72/72 [==============================] - 4s 56ms/step - loss: 0.2146 - accuracy: 0.9217 - val_loss: 0.1345 - val_accuracy: 0.9550\n",
      "Epoch 7/20\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.1670 - accuracy: 0.9433 - val_loss: 0.1052 - val_accuracy: 0.9750\n",
      "Epoch 8/20\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.1679 - accuracy: 0.9372 - val_loss: 0.0936 - val_accuracy: 0.9700\n",
      "Epoch 9/20\n",
      "72/72 [==============================] - 4s 58ms/step - loss: 0.0975 - accuracy: 0.9703 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "72/72 [==============================] - 4s 57ms/step - loss: 0.0992 - accuracy: 0.9700 - val_loss: 0.0505 - val_accuracy: 0.9950\n",
      "Epoch 11/20\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.0946 - accuracy: 0.9658 - val_loss: 0.1403 - val_accuracy: 0.9400\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3876 - accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333134651184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = 'min',\\\n",
    "                                           patience = 2, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "history_ht = model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.1, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, HTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "HTypred = model.predict(X_ttT)\n",
    "HTypred = tf.argmax(input = HTypred, axis = 1).numpy()\n",
    "HTypred = encoder.inverse_transform(HTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "display(SCypred)\n",
    "CMSC = confusion_matrix(ytruth, SCypred, labels = labels)\n",
    "CMFT = confusion_matrix(ytruth, FTypred, labels = labels)\n",
    "CMHT = confusion_matrix(ytruth, HTypred, labels = labels)\n",
    "display(SCaccuracy, FTaccuracy, HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharey = True, figsize = [15, 5])\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "pcm = axs[0].pcolor(CMSC, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[0].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_yticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_ylabel(\"Actual Condition\")\n",
    "axs[0].set_xlabel(\"Predicted Condition\")\n",
    "axs[0].xaxis.set_label_position('top') \n",
    "axs[0].set_title('Raw Data');\n",
    "\n",
    "axs[1].pcolor(CMFT, edgecolors = 'k', cmap = 'gist_heat_r');\n",
    "plt.gca().invert_yaxis()\n",
    "axs[1].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[1].set_title('Fourier Transform');\n",
    "axs[1].set_xlabel(\"Predicted Condition\")\n",
    "axs[1].xaxis.set_label_position('top')\n",
    "\n",
    "axs[2].pcolor(CMHT, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[2].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels);\n",
    "axs[2].set_title('Walsh Hadamard Transform');\n",
    "axs[2].set_xlabel(\"Predicted Condition\")\n",
    "axs[2].xaxis.set_label_position('top')\n",
    "\n",
    "fig.colorbar(pcm, ax = axs[:], location = 'bottom', label = 'Number of Assignments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d30a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = history_sc.history\n",
    "train_loss = hist_dict['loss']\n",
    "val_loss = hist_dict['val_loss']\n",
    "train_acc = hist_dict['accuracy']\n",
    "val_acc = hist_dict['val_accuracy']\n",
    "epochs_vec = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "#\n",
    "# Plot the model accuracy vs Epochs\n",
    "#\n",
    "ax[0].plot(epochs_vec, train_acc, 'bo')\n",
    "ax[0].plot(epochs_vec, val_acc, 'b')\n",
    "#\n",
    "# Plot the loss vs Epochs\n",
    "#\n",
    "ax[1].plot(epochs_vec, train_loss, 'bo')\n",
    "ax[1].plot(epochs_vec, val_loss, 'b')\n",
    "\n",
    "hist_dict = history_ft.history\n",
    "train_loss = hist_dict['loss']\n",
    "val_loss = hist_dict['val_loss']\n",
    "train_acc = hist_dict['accuracy']\n",
    "val_acc = hist_dict['val_accuracy']\n",
    "epochs_vec = range(1, len(train_loss) + 1)\n",
    "\n",
    "#\n",
    "# Plot the model accuracy vs Epochs\n",
    "#\n",
    "ax[0].plot(epochs_vec, train_acc, 'ko')\n",
    "ax[0].plot(epochs_vec, val_acc, 'k')\n",
    "\n",
    "#\n",
    "# Plot the loss vs Epochs\n",
    "#\n",
    "ax[1].plot(epochs_vec, train_loss, 'ko')\n",
    "ax[1].plot(epochs_vec, val_loss, 'k')\n",
    "\n",
    "hist_dict = history_ht.history\n",
    "train_loss = hist_dict['loss']\n",
    "val_loss = hist_dict['val_loss']\n",
    "train_acc = hist_dict['accuracy']\n",
    "val_acc = hist_dict['val_accuracy']\n",
    "epochs_vec = range(1, len(train_loss) + 1)\n",
    "\n",
    "#\n",
    "# Plot the model accuracy vs Epochs\n",
    "#\n",
    "ax[0].plot(epochs_vec, train_acc, 'ro')\n",
    "ax[0].plot(epochs_vec, val_acc, 'r')\n",
    "ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Scaled Training', 'Scaled Validation', 'Fourier Training', 'Fourier Validation', 'Hadamard Training', 'Hadamard Validation'])\n",
    "#\n",
    "# Plot the loss vs Epochs\n",
    "#\n",
    "ax[1].plot(epochs_vec, train_loss, 'ro')\n",
    "ax[1].plot(epochs_vec, val_loss, 'r')\n",
    "ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Scaled Training', 'Scaled Validation', 'Fourier Training', 'Fourier Validation', 'Hadamard Training', 'Hadamard Validation']);\n",
    "\n",
    "plt.savefig(\"Tables and Figures\\SVG Files\\CNN Learning Curve.svg\", format = \"svg\", bbox_inches='tight')\n",
    "plt.savefig(\"Tables and Figures\\PNG Files\\CNN Learning Curve.png\", format = \"png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc768e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
