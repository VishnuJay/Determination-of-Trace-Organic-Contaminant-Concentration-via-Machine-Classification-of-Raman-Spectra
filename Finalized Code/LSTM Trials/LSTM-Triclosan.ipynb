{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5953c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import KeyFunctions as me\n",
    "import tensorflow as tf\n",
    "RandState = 117\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b201c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Full Triclosan Dataset\n",
    "df, labels = me.ConstructCombinedTriclosanDataset()\n",
    "\n",
    "[train, test] = train_test_split(df, random_state = RandState, shuffle = True, train_size = 0.8)\n",
    "\n",
    "y_tn = train.index\n",
    "y_tt = test.index\n",
    "X_tt = test.to_numpy()\n",
    "X_tn = train.to_numpy()\n",
    "\n",
    "#Augment Data to 4000 Spectra\n",
    "X_tnAu, y_tnAu = me.AugmentData(X_tn, y_tn, 4000, df.columns.to_numpy(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187a01c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-5    11\n",
       "10-3    10\n",
       "10-4    10\n",
       "10-7     6\n",
       "10-8     6\n",
       "10-9     6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdef8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Training Parameters\n",
    "verbose = 1\n",
    "epochsvec = [5, 20, 50]\n",
    "batch_sizevec = [10, 50, 100]\n",
    "epochs = epochsvec[1]\n",
    "batch_size = batch_sizevec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479b91a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 1021, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1021, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnAu)\n",
    "X_tnS = scaler.transform(X_tnAu)\n",
    "X_ttS = scaler.transform(X_tt)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnS.reshape(X_tnS.shape[0], X_tnS.shape[1], 1)\n",
    "X_tt_p = X_ttS.reshape(X_ttS.shape[0], X_ttS.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)\n",
    "\n",
    "ytruth = tf.argmax(input = y_ttT, axis = 1).numpy()\n",
    "ytruth = encoder.inverse_transform(ytruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374fecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 0.2774 - accuracy: 0.8975 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 7s 113ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 8s 125ms/step - loss: 5.9888e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 8s 123ms/step - loss: 2.9581e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 1.8533e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 8s 125ms/step - loss: 1.2113e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 8s 123ms/step - loss: 8.5924e-05 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 6.2642e-05 - accuracy: 1.0000 - val_loss: 8.6075e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 4.8994e-05 - accuracy: 1.0000 - val_loss: 7.2357e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 4.0442e-05 - accuracy: 1.0000 - val_loss: 6.1953e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 3.1012e-05 - accuracy: 1.0000 - val_loss: 5.4123e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 2.6379e-05 - accuracy: 1.0000 - val_loss: 4.8037e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 2.1999e-05 - accuracy: 1.0000 - val_loss: 4.2170e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 1.8988e-05 - accuracy: 1.0000 - val_loss: 3.7690e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 8s 121ms/step - loss: 1.6358e-05 - accuracy: 1.0000 - val_loss: 3.4098e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 8s 125ms/step - loss: 1.4586e-05 - accuracy: 1.0000 - val_loss: 3.1027e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 8s 132ms/step - loss: 1.2661e-05 - accuracy: 1.0000 - val_loss: 2.8154e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 8s 127ms/step - loss: 1.1364e-05 - accuracy: 1.0000 - val_loss: 2.5602e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 9.6553e-06 - accuracy: 1.0000 - val_loss: 2.3646e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 6s 99ms/step - loss: 8.4182e-06 - accuracy: 1.0000 - val_loss: 2.1888e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.3490 - accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.800000011920929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(100 ,input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_split = 0.2, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, SCaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(SCaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ccf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "SCypred = model.predict(X_ttT)\n",
    "SCypred = tf.argmax(input = SCypred, axis = 1).numpy()\n",
    "SCypred = encoder.inverse_transform(SCypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b7e0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , ..., 0.00570848,\n",
       "         0.        , 0.01431662],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.00806746],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.01108859],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.0067522 ,\n",
       "         0.        , 0.01469975],\n",
       "        [0.        , 0.        , 0.        , ..., 0.00048371,\n",
       "         0.        , 0.0123987 ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.00166608,\n",
       "         0.        , 0.01283272]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extract Convolution Feature Maps\n",
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec = list()\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "SCfeature_maps = convlayer.predict(spectra)\n",
    "display(SCfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53fd3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 2042, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 2042, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply Fourier Transform to Training and Testing Data\n",
    "X_tnf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tnAu)\n",
    "X_ttf = np.apply_along_axis(np.fft.fft, axis=1, arr=X_tt)\n",
    "\n",
    "#Combine Real and Imaginary Part of FT in form [real, imaginary]\n",
    "X_tnf = np.append(X_tnf.real, X_tnf.imag, axis = 1)\n",
    "X_ttf = np.append(X_ttf.real, X_ttf.imag, axis = 1)\n",
    "X_tnf= X_tnf.astype('float32')\n",
    "X_ttf= X_ttf.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnf)\n",
    "X_tnf = scaler.transform(X_tnf)\n",
    "X_ttf = scaler.transform(X_ttf)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnf.reshape(X_tnf.shape[0], X_tnf.shape[1], 1)\n",
    "X_tt_p = X_ttf.reshape(X_ttf.shape[0], X_ttf.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e552db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 0.2859 - accuracy: 0.9153 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 4.3260e-04 - accuracy: 1.0000 - val_loss: 8.3890e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 13s 198ms/step - loss: 2.5570e-04 - accuracy: 1.0000 - val_loss: 5.8124e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 1.7116e-04 - accuracy: 1.0000 - val_loss: 4.2642e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 1.2378e-04 - accuracy: 1.0000 - val_loss: 3.3079e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 9.1072e-05 - accuracy: 1.0000 - val_loss: 2.6077e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 12s 193ms/step - loss: 7.0176e-05 - accuracy: 1.0000 - val_loss: 2.1429e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 12s 190ms/step - loss: 5.6737e-05 - accuracy: 1.0000 - val_loss: 1.7886e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 10s 162ms/step - loss: 4.6168e-05 - accuracy: 1.0000 - val_loss: 1.5293e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 10s 159ms/step - loss: 3.8711e-05 - accuracy: 1.0000 - val_loss: 1.2986e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 3.1946e-05 - accuracy: 1.0000 - val_loss: 1.1201e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 11s 177ms/step - loss: 2.6898e-05 - accuracy: 1.0000 - val_loss: 9.8834e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 12s 184ms/step - loss: 2.3503e-05 - accuracy: 1.0000 - val_loss: 8.7600e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 12s 195ms/step - loss: 2.0395e-05 - accuracy: 1.0000 - val_loss: 7.8803e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 1.7812e-05 - accuracy: 1.0000 - val_loss: 7.0998e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 12s 194ms/step - loss: 1.5626e-05 - accuracy: 1.0000 - val_loss: 6.4570e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 13s 197ms/step - loss: 1.4040e-05 - accuracy: 1.0000 - val_loss: 5.8829e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 12s 195ms/step - loss: 1.2595e-05 - accuracy: 1.0000 - val_loss: 5.3549e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 12s 194ms/step - loss: 1.1196e-05 - accuracy: 1.0000 - val_loss: 4.8777e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 12s 194ms/step - loss: 1.0278e-05 - accuracy: 1.0000 - val_loss: 4.5139e-05 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.9353 - accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.699999988079071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(100 ,input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",patience = 2,verbose = 0, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.2, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, FTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(FTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54173aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Prediction\n",
    "FTypred = model.predict(X_ttT)\n",
    "FTypred = tf.argmax(input = FTypred, axis = 1).numpy()\n",
    "FTypred = encoder.inverse_transform(FTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0580f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2_input (InputLayer)  [(None, 2042, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2042, 64)          128       \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.03675821, 0.        , ..., 0.        ,\n",
       "         0.0754616 , 0.0374639 ],\n",
       "        [0.0085915 , 0.        , 0.08131342, ..., 0.00609886,\n",
       "         0.        , 0.00981886],\n",
       "        [0.        , 0.03301616, 0.        , ..., 0.        ,\n",
       "         0.0696957 , 0.03616336],\n",
       "        ...,\n",
       "        [0.        , 0.00961833, 0.        , ..., 0.        ,\n",
       "         0.03364341, 0.02803145],\n",
       "        [0.01006394, 0.        , 0.08898406, ..., 0.00683265,\n",
       "         0.        , 0.00856336],\n",
       "        [0.        , 0.02495755, 0.        , ..., 0.        ,\n",
       "         0.05727868, 0.03336259]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "convlayer.summary()\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "FTfeature_maps = convlayer.predict(spectra)\n",
    "display(FTfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "756a0320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 1024, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4000, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1024, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply Welsh-Hadamard Transform to Training and Testing Data\n",
    "from sympy.discrete.transforms import fwht, ifwht\n",
    "X_tnh = np.apply_along_axis(fwht, axis=1, arr=X_tnAu)\n",
    "X_tth = np.apply_along_axis(fwht, axis=1, arr=X_tt)\n",
    "X_tnh = X_tnh.astype('float32')\n",
    "X_tth = X_tth.astype('float32')\n",
    "\n",
    "#Scale X-Data with Training Xs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tnh)\n",
    "X_tnh = scaler.transform(X_tnh)\n",
    "X_tth = scaler.transform(X_tth)\n",
    "\n",
    "#Encode y-Data with Training ys\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_tnAu)\n",
    "y_tn_e = encoder.transform(y_tnAu)\n",
    "y_tn_p = np_utils.to_categorical(y_tn_e, num_classes = len(labels))\n",
    "y_tt_e = encoder.transform(y_tt)\n",
    "y_tt_p = np_utils.to_categorical(y_tt_e, num_classes = len(labels))\n",
    "\n",
    "#Reshape All Data to a 3D Tensor of Shape [Number of Spectra, Number of Timesteps(1), Number of Wavelengths]\n",
    "X_tn_p = X_tnh.reshape(X_tnh.shape[0], X_tnh.shape[1], 1)\n",
    "X_tt_p = X_tth.reshape(X_tth.shape[0], X_tth.shape[1], 1)\n",
    "\n",
    "y_tnT = tf.convert_to_tensor(y_tn_p)\n",
    "y_ttT = tf.convert_to_tensor(y_tt_p)\n",
    "X_tnT = tf.convert_to_tensor(X_tn_p)\n",
    "X_ttT = tf.convert_to_tensor(X_tt_p)\n",
    "\n",
    "\n",
    "display(X_tnT.shape)\n",
    "display(y_tnT.shape)\n",
    "display(X_ttT.shape)\n",
    "display(y_ttT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e029d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 8s 126ms/step - loss: 0.2432 - accuracy: 0.9237 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 6s 92ms/step - loss: 3.2125e-04 - accuracy: 1.0000 - val_loss: 4.9429e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 7s 104ms/step - loss: 1.0644e-04 - accuracy: 1.0000 - val_loss: 2.1059e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 6s 99ms/step - loss: 4.4619e-05 - accuracy: 1.0000 - val_loss: 1.1315e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 8s 131ms/step - loss: 2.3944e-05 - accuracy: 1.0000 - val_loss: 7.0292e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 9s 138ms/step - loss: 1.5478e-05 - accuracy: 1.0000 - val_loss: 4.9813e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 9s 140ms/step - loss: 1.0930e-05 - accuracy: 1.0000 - val_loss: 3.6307e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 9s 138ms/step - loss: 7.9560e-06 - accuracy: 1.0000 - val_loss: 2.8644e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 9s 140ms/step - loss: 6.3796e-06 - accuracy: 1.0000 - val_loss: 2.2412e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 9s 140ms/step - loss: 5.0562e-06 - accuracy: 1.0000 - val_loss: 1.8036e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 9s 143ms/step - loss: 4.0850e-06 - accuracy: 1.0000 - val_loss: 1.5313e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 9s 143ms/step - loss: 3.5075e-06 - accuracy: 1.0000 - val_loss: 1.3121e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 9s 143ms/step - loss: 2.9206e-06 - accuracy: 1.0000 - val_loss: 1.1262e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 9s 142ms/step - loss: 2.5005e-06 - accuracy: 1.0000 - val_loss: 9.6964e-06 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 9s 144ms/step - loss: 2.2903e-06 - accuracy: 1.0000 - val_loss: 8.4647e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 9s 142ms/step - loss: 1.9816e-06 - accuracy: 1.0000 - val_loss: 7.5687e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 9s 138ms/step - loss: 1.6953e-06 - accuracy: 1.0000 - val_loss: 6.5760e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 9s 139ms/step - loss: 1.6425e-06 - accuracy: 1.0000 - val_loss: 6.1528e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 9s 141ms/step - loss: 1.4265e-06 - accuracy: 1.0000 - val_loss: 5.3459e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 10s 149ms/step - loss: 1.2561e-06 - accuracy: 1.0000 - val_loss: 5.0069e-06 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.2268 - accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.800000011920929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multi-class Classification with Keras\n",
    " \n",
    "n_timesteps, n_features, n_outputs = X_tn_p.shape[1], X_tn_p.shape[2], y_tn_p.shape[1]\n",
    "\n",
    "#Define Sequential Model - 1 Convolution Layer, 1 Dropout Layer, 1 Flatten Layer, 2 Dense Layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(100 ,input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Implement EarlyStopping\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = 'min',\\\n",
    "                                           patience = 2, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(X_tnT, y_tnT, epochs=epochs, batch_size=batch_size, verbose=verbose,  validation_split=0.2, callbacks = stopper)\n",
    "\n",
    "#Evaluate Model\n",
    "_, HTaccuracy = model.evaluate(X_ttT, y_ttT, batch_size=batch_size, verbose=verbose)\n",
    "display(HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0d1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000204C532AAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "#Make Prediction\n",
    "HTypred = model.predict(X_ttT)\n",
    "HTypred = tf.argmax(input = HTypred, axis = 1).numpy()\n",
    "HTypred = encoder.inverse_transform(HTypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13ca327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4_input (InputLayer)  [(None, 1024, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1024, 64)          128       \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000204C532A3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.01704791, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.00112041, ..., 0.        ,\n",
       "         0.00772193, 0.        ],\n",
       "        [0.        , 0.13022918, 0.18318856, ..., 0.09956384,\n",
       "         0.1340554 , 0.11658569],\n",
       "        ...,\n",
       "        [0.        , 0.00176172, 0.01259788, ..., 0.00608204,\n",
       "         0.01568592, 0.        ],\n",
       "        [0.0277566 , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.02583339, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convlayer = tf.keras.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "convlayer.summary()\n",
    "\n",
    "spectra = X_ttT[n, :, 0]\n",
    "specvec.append(spectra)\n",
    "spectra = tf.reshape(spectra, (1, len(spectra), 1))\n",
    "\n",
    "HTfeature_maps = convlayer.predict(spectra)\n",
    "display(HTfeature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0107edb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10-5', '10-9', '10-7', '10-7', '10-4', '10-3', '10-8', '10-4',\n",
       "       '10-3', '10-9'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.800000011920929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.699999988079071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.800000011920929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "display(SCypred)\n",
    "CMSC = confusion_matrix(ytruth, SCypred, labels = labels)\n",
    "CMFT = confusion_matrix(ytruth, FTypred, labels = labels)\n",
    "CMHT = confusion_matrix(ytruth, HTypred, labels = labels)\n",
    "display(SCaccuracy, FTaccuracy, HTaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c1b1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharey = True, figsize = [15, 5])\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "pcm = axs[0].pcolor(CMSC, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[0].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_yticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[0].set_ylabel(\"Actual Condition\")\n",
    "axs[0].set_xlabel(\"Predicted Condition\")\n",
    "axs[0].xaxis.set_label_position('top') \n",
    "axs[0].set_title('Raw Data');\n",
    "\n",
    "axs[1].pcolor(CMFT, edgecolors = 'k', cmap = 'gist_heat_r');\n",
    "plt.gca().invert_yaxis()\n",
    "axs[1].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels)\n",
    "axs[1].set_title('Fourier Transform');\n",
    "axs[1].set_xlabel(\"Predicted Condition\")\n",
    "axs[1].xaxis.set_label_position('top')\n",
    "\n",
    "axs[2].pcolor(CMHT, edgecolors = 'k', cmap = 'gist_heat_r')\n",
    "plt.gca().invert_yaxis()\n",
    "axs[2].set_xticks(ticks = np.linspace(0.5, len(labels)-0.5, num = len(labels)), labels = labels);\n",
    "axs[2].set_title('Walsh Hadamard Transform');\n",
    "axs[2].set_xlabel(\"Predicted Condition\")\n",
    "axs[2].xaxis.set_label_position('top')\n",
    "\n",
    "fig.colorbar(pcm, ax = axs[:], location = 'bottom', label = 'Number of Assignments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d30a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
